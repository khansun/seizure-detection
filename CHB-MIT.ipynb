{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gm9X27QALJzE"
   },
   "source": [
    "# *Pre-processing and Feature Extraction*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyedflib\n",
    "import tables\n",
    "from urllib.request import urlretrieve\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to load data from an EDF file\n",
    "def data_load(filename, selected_channels=[]):\n",
    "    try:\n",
    "\n",
    "        f = pyedflib.EdfReader(filename)\n",
    "        # os.remove(filename)\n",
    "\n",
    "        if len(selected_channels) == 0:\n",
    "            selected_channels = f.getSignalLabels()\n",
    "\n",
    "        channel_names = f.getSignalLabels()\n",
    "        channel_freq = f.getSampleFrequencies()\n",
    "\n",
    "        sigbufs = np.zeros((f.getNSamples()[0], len(selected_channels)))\n",
    "        for i, channel in enumerate(selected_channels):\n",
    "            sigbufs[:, i] = f.readSignal(channel_names.index(channel))\n",
    "\n",
    "        df = pd.DataFrame(sigbufs, columns=selected_channels).astype('float32')\n",
    "        index_increase = np.linspace(0, len(df) / channel_freq[0], len(df), endpoint=False)\n",
    "        seconds = np.floor(index_increase).astype('uint16')\n",
    "        df['Time'] = seconds\n",
    "        df = df.set_index('Time')\n",
    "        df.columns.name = 'Channel'\n",
    "\n",
    "        return df, channel_freq[0]\n",
    "\n",
    "    except OSError as e:\n",
    "        print(f\"Error loading EDF file {file}: {e}\")\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseSummaryDf(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    file_names = []\n",
    "    start_times = []\n",
    "    end_times = []\n",
    "    num_seizures = []\n",
    "    seizure_start_times = []\n",
    "    seizure_end_times = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "\n",
    "        if line.startswith(\"File Name:\"):\n",
    "            file_name = line.split(\":\")[1].strip()\n",
    "            file_names.append(file_name)\n",
    "\n",
    "            i += 1\n",
    "            start_time = lines[i].split(\":\")[1].strip()\n",
    "            start_times.append(start_time)\n",
    "\n",
    "            i += 1\n",
    "            end_time = lines[i].split(\":\")[1].strip()\n",
    "            end_times.append(end_time)\n",
    "\n",
    "            i += 1\n",
    "            seizures = int(lines[i].split(\":\")[1].strip())\n",
    "            num_seizures.append(seizures)\n",
    "\n",
    "            if seizures > 0:\n",
    "                i += 1\n",
    "                start_seizure = int(lines[i].split(\":\")[1].strip().replace(\" seconds\", \"\"))\n",
    "                seizure_start_times.append(start_seizure)\n",
    "\n",
    "                i += 1\n",
    "                end_seizure = int(lines[i].split(\":\")[1].strip().replace(\" seconds\", \"\"))\n",
    "                seizure_end_times.append(end_seizure)\n",
    "            else:\n",
    "                seizure_start_times.append(None)\n",
    "                seizure_end_times.append(None)\n",
    "        i += 1\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'File Name': file_names,\n",
    "        'Start Time': start_times,\n",
    "        'End Time': end_times,\n",
    "        'Number of Seizures': num_seizures,\n",
    "        'Seizure Start Time': seizure_start_times,\n",
    "        'Seizure End Time': seizure_end_times\n",
    "    })\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def add_class_column(summaryRow, data_dir):\n",
    "    file_name = re.sub(\".edf\", \".csv\", summaryRow['File Name'])\n",
    "    csv_path = f\"{data_dir}/{file_name}\"\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        data = pd.read_csv(csv_path)\n",
    "\n",
    "        # Create a mask for times within the seizure window\n",
    "        time_within_seizure = (\n",
    "            (data['Time'] >= summaryRow['Seizure Start Time']) & \n",
    "            (data['Time'] <= summaryRow['Seizure End Time'])\n",
    "        )\n",
    "        \n",
    "        # Assign the 'Class' column based on the mask\n",
    "        data['Class'] = time_within_seizure.astype(int)  # 1 for within seizure, 0 otherwise\n",
    "        data.to_csv(csv_path)\n",
    "        return data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {csv_path}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if file is not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset/chb-mit-scalp/chb01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file processed: chb01_12.edf frequency: 256.0\n",
      "file processed: chb01_10.edf frequency: 256.0\n",
      "file processed: chb01_33.edf frequency: 256.0\n",
      "file processed: chb01_02.edf frequency: 256.0\n",
      "file processed: chb01_22.edf frequency: 256.0\n",
      "file processed: chb01_32.edf frequency: 256.0\n",
      "file processed: chb01_29.edf frequency: 256.0\n",
      "file processed: chb01_41.edf frequency: 256.0\n",
      "file processed: chb01_25.edf frequency: 256.0\n",
      "file processed: chb01_04.edf frequency: 256.0\n",
      "file processed: chb01_16.edf frequency: 256.0\n",
      "file processed: chb01_42.edf frequency: 256.0\n",
      "error <class 'Exception'> \n",
      "error <class 'Exception'> \n",
      "error <class 'Exception'> \n",
      "file processed: chb01_30.edf frequency: 256.0\n",
      "error <class 'Exception'> \n",
      "file processed: chb01_20.edf frequency: 256.0\n",
      "error <class 'Exception'> \n",
      "file processed: chb01_15.edf frequency: 256.0\n",
      "file processed: chb01_23.edf frequency: 256.0\n",
      "file processed: chb01_01.edf frequency: 256.0\n",
      "error <class 'Exception'> \n",
      "file processed: chb01_38.edf frequency: 256.0\n",
      "file processed: chb01_05.edf frequency: 256.0\n",
      "file processed: chb01_34.edf frequency: 256.0\n",
      "file processed: chb01_31.edf frequency: 256.0\n",
      "error <class 'Exception'> \n",
      "error <class 'Exception'> \n",
      "error <class 'Exception'> \n",
      "file processed: chb01_06.edf frequency: 256.0\n",
      "error <class 'Exception'> \n",
      "file processed: chb01_14.edf frequency: 256.0\n",
      "file processed: chb01_11.edf frequency: 256.0\n",
      "file processed: chb01_26.edf frequency: 256.0\n"
     ]
    }
   ],
   "source": [
    "for part_file in os.listdir(data_path):\n",
    "    try:      \n",
    "        df, freq = data_load(data_path+\"/\"+part_file)\n",
    "        df.to_csv(re.sub(\".edf\", \".csv\", data_path+\"/\"+part_file))\n",
    "        print(f\"file processed: {part_file} frequency: {freq}\")\n",
    "    except Exception:\n",
    "        print (f\"error {Exception} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryDf = parseSummaryDf('./dataset/chb-mit-scalp/chb01/chb01-summary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 3:\n",
      "        Unnamed: 0  Time     FP1-F7      F7-T7      T7-P7      P7-O1  \\\n",
      "0                0     0  93.186810  57.240536 -77.948715  -2.539683   \n",
      "1                1     0   0.195360   0.195360   0.195360   0.195360   \n",
      "2                2     0   0.195360   0.195360   0.195360   0.195360   \n",
      "3                3     0  -0.195360   0.195360   0.195360   0.195360   \n",
      "4                4     0   0.976801  -0.195360   0.195360   0.195360   \n",
      "...            ...   ...        ...        ...        ...        ...   \n",
      "921595      921595  3599 -18.949940  -1.367521  -1.758242  12.307693   \n",
      "921596      921596  3599 -13.870574  -0.976801  -3.321123   2.148962   \n",
      "921597      921597  3599  -2.930403  -0.195360  -9.963370   2.148962   \n",
      "921598      921598  3599   4.493285   1.367521 -11.526252  -0.976801   \n",
      "921599      921599  3599  11.526252  -3.321123 -11.916972  -2.148962   \n",
      "\n",
      "           FP1-F3      F3-C3      C3-P3      P3-O1  ...     T8-P8      P8-O2  \\\n",
      "0       78.339440  20.512821 -15.433455 -13.479854  ... -2.930403  44.346764   \n",
      "1        0.195360   0.195360   0.195360   0.195360  ...  0.195360   0.195360   \n",
      "2        0.195360   0.195360   0.195360   0.195360  ...  0.195360  -0.195360   \n",
      "3       -0.195360   0.195360   0.195360   0.586081  ...  0.195360  -1.367521   \n",
      "4        0.976801   0.195360   0.976801  -0.976801  ... -0.195360   3.321123   \n",
      "...           ...        ...        ...        ...  ...       ...        ...   \n",
      "921595  -9.572650 -10.354091   4.884005   5.274725  ...  0.586081   3.321123   \n",
      "921596  -7.228327  -7.228327   5.665446  -6.446886  ...  1.758242   3.321123   \n",
      "921597  -2.930403  -0.586081   0.586081  -8.400489  ...  4.884005   0.195360   \n",
      "921598  -1.367521   6.056166  -1.758242  -9.181930  ...  6.837607  -6.837607   \n",
      "921599   0.976801   6.446886   3.321123 -17.777779  ...  4.493285   6.837607   \n",
      "\n",
      "            FZ-CZ      CZ-PZ      P7-T7     T7-FT9   FT9-FT10    FT10-T8  \\\n",
      "0      -30.280830  58.803417  78.339440 -18.168499 -56.849815 -15.433455   \n",
      "1        0.195360   0.195360   0.195360   0.195360   0.195360   0.195360   \n",
      "2        0.195360   0.195360   0.195360   0.195360   0.195360   0.195360   \n",
      "3        0.195360   0.195360   0.195360  -0.195360   0.195360   0.195360   \n",
      "4        0.195360   0.976801   0.195360   2.148962  -0.586081  -0.195360   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "921595  19.731380  -1.758242   2.148962 -10.354091  27.545788   4.493285   \n",
      "921596  18.559220   1.367521   3.711844  -3.711844  15.433455   5.274725   \n",
      "921597  15.433455  -1.367521  10.354091   0.586081  -9.572650   6.446886   \n",
      "921598  15.433455   0.195360  11.916972   0.976801 -24.420025   9.963370   \n",
      "921599  16.605618   7.619048  12.307693  13.479854 -37.313797   8.791209   \n",
      "\n",
      "         T8-P8.1  Class  \n",
      "0      -2.930403      0  \n",
      "1       0.195360      0  \n",
      "2       0.195360      0  \n",
      "3       0.195360      0  \n",
      "4      -0.195360      0  \n",
      "...          ...    ...  \n",
      "921595  0.586081      0  \n",
      "921596  1.758242      0  \n",
      "921597  4.884005      0  \n",
      "921598  6.837607      0  \n",
      "921599  4.493285      0  \n",
      "\n",
      "[921600 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "for index, row in summaryDf.iterrows():\n",
    "    if(index == 3):\n",
    "        print(f\"Row {index}: {row['File Name']}\" )\n",
    "        df = add_class_column(row, './dataset/chb-mit-scalp/chb01')\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Process and Export Feature Dump*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyWavelets in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from PyWavelets) (2.1.2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import welch\n",
    "from pywt import wavedec, swt\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "from numpy import inf\n",
    "import math, os, glob, re, tables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "def window(a, w, o, copy = False):\n",
    "    # if there is no window to be applied\n",
    "    if w == None:\n",
    "        view = np.expand_dims(a.T, axis=0)\n",
    "        \n",
    "    # otherwise...\n",
    "    else:\n",
    "    \n",
    "        sh = (a.size - w + 1, w)\n",
    "        st = a.strides * 2\n",
    "        if o:\n",
    "            view = np.lib.stride_tricks.as_strided(a, strides = st, shape = sh)[0::o]\n",
    "        else:\n",
    "            view = np.lib.stride_tricks.as_strided(a, strides = st, shape = sh)[0::w]\n",
    "    if copy:\n",
    "        return view.copy()\n",
    "    else:\n",
    "        return view\n",
    "\n",
    "def window_y(events, window_size, overlap, target=None, baseline=None):\n",
    "    \n",
    "  # window the data so each row is another epoch\n",
    "  events_windowed = window(events, w = window_size, o = overlap, copy = True)\n",
    "  \n",
    "  if target:\n",
    "    # turn to array of bools if seizure in the\n",
    "    # windowed data\n",
    "    bools = events_windowed == target\n",
    "    # are there any seizure seconds in the data?\n",
    "    data_y = np.any(bools,axis=1)\n",
    "    # turn to 0's and 1's\n",
    "    data_y = data_y.astype(int)\n",
    "    # expand the dimensions so running down one column\n",
    "    data_y = np.expand_dims(data_y, axis=1)\n",
    "  \n",
    "  elif baseline:\n",
    "    # replace all baseline labels to nan\n",
    "    data_y = pd.DataFrame(events_windowed).replace(baseline, np.nan)\n",
    "    # get the most common other than baseline\n",
    "    data_y = data_y.mode(1)\n",
    "    # change nan back to baseline class\n",
    "    data_y = data_y.fillna(baseline).values\n",
    "    # if there was nothing but baseline there will be an empty array\n",
    "    if data_y.size == 0:\n",
    "        data_y = np.array([baseline]*data_y.shape[0])\n",
    "        data_y = np.expand_dims(data_y, -1)\n",
    "  \n",
    "  else:\n",
    "    # get the value most frequent in the window\n",
    "    data_y = pd.DataFrame(events_windowed).mode(1).values\n",
    "\n",
    "  return data_y\n",
    "    \n",
    "    \n",
    "def bandpower(data, sf, weighted, mean, band):\n",
    "    low, high = band\n",
    "    \n",
    "    # TODO: Not sure this does much...\n",
    "    if weighted:\n",
    "        weighted_window = ('tukey', 3)\n",
    "        \n",
    "    else:\n",
    "        weighted_window = 'hann'\n",
    "\n",
    "    # Compute the periodogram (Welch)\n",
    "    freqs, psd = welch(data,\n",
    "                       sf,\n",
    "                       window = weighted_window,\n",
    "                       nperseg=(2 / low)*sf, # this ensures there are at least 2 cycles of the lowest frequency in the window\n",
    "                       scaling='density',\n",
    "                       axis=0\n",
    "                      )\n",
    "    \n",
    "    # Find closest indices of band in frequency vector\n",
    "    idx_min = np.argmax(np.round(freqs) > low) - 1\n",
    "    idx_max = np.argmax(np.round(freqs) > high)\n",
    "    \n",
    "    #select frequencies of interest\n",
    "    psd = psd[idx_min:idx_max,:]\n",
    "    \n",
    "    if mean:\n",
    "        psd = np.nanmean(psd,0)\n",
    "    else:\n",
    "        psd = np.nanmedian(psd,0)\n",
    "    \n",
    "    return psd\n",
    "\n",
    "\n",
    "def feature_append(all_features, data, axis=1, expand=True):\n",
    "    if expand:\n",
    "        data = np.expand_dims(data, axis=axis)\n",
    "    \n",
    "    # if the feature set is empty\n",
    "    if all_features.size == 0:\n",
    "        all_features = data\n",
    "    else:\n",
    "        all_features = np.concatenate((all_features, data), axis)\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "\n",
    "def pad_along_axis(array, target_length, axis=0):\n",
    "\n",
    "    pad_size = target_length - array.shape[axis]\n",
    "    axis_nb = len(array.shape)\n",
    "\n",
    "    if pad_size < 0:\n",
    "        return array\n",
    "\n",
    "    npad = [(0, 0) for x in range(axis_nb)]\n",
    "    npad[axis] = (0, pad_size)\n",
    "\n",
    "    b = np.pad(array, pad_width=npad, mode='constant', constant_values=0)\n",
    "\n",
    "    return b\n",
    "\n",
    "\n",
    "def wavelet_decompose(data, feature_list, channel_name=None, wavelet='db4', wavelet_transform = 'DWT', level=6, scale=False):\n",
    "    # bool to check if the data has been padded\n",
    "    padded = False\n",
    "    \n",
    "    if wavelet_transform == 'DWT':\n",
    "        # get the wavelet coefficients at each level in a list\n",
    "        coeffs_list = wavedec(data, wavelet=wavelet, level=level)\n",
    "    \n",
    "    elif wavelet_transform == 'UDWT':\n",
    "        # The signal length along the transformed axis be a multiple of 2**level\n",
    "        atrous = (2**level)\n",
    "        orig_shape = data.shape\n",
    "        ceiled_len = math.ceil(orig_shape[1]/atrous)\n",
    "        if orig_shape[1]/atrous != ceiled_len:\n",
    "            padded_len = ceiled_len*atrous\n",
    "            data = pad_along_axis(data, padded_len, axis=1)\n",
    "            padded = True\n",
    "        # get the wavelet coefficients at each level in a list\n",
    "        coeffs_list = swt(data, wavelet=wavelet, level=level)\n",
    "    \n",
    "    if channel_name:\n",
    "        # make an empty list for the features later\n",
    "        feature_names = []\n",
    "        \n",
    "        # make a list of the component names\n",
    "        nums = list(range(1,level+1))\n",
    "        names=[]\n",
    "        for num in nums:\n",
    "            names.append('D' + str(num))\n",
    "        # reverse the names so it counts down\n",
    "        names = names[::-1]  \n",
    "    \n",
    "    # make empty arrays to help store data in later\n",
    "    mean = np.array([])\n",
    "    std = np.array([])\n",
    "    LSWT = np.array([])\n",
    "    mean_abs = np.array([])\n",
    "    \n",
    "    # for each decomposition level from the wavelets...\n",
    "    for i, array in enumerate(coeffs_list):\n",
    "        if wavelet_transform == 'DWT':\n",
    "            # skip the first A\n",
    "            if i == 0:\n",
    "                continue\n",
    "        elif wavelet_transform == 'UDWT':\n",
    "            # just get the D's\n",
    "            array = array[1]\n",
    "            if padded:\n",
    "                # remove the padding\n",
    "                array = array[:orig_shape[0],:orig_shape[1]]\n",
    "              \n",
    "        if 'mean' in feature_list:\n",
    "            # add the data straight into the wavelet_features array\n",
    "            mean = feature_append(mean, np.mean(array,1))\n",
    "        if 'std' in feature_list:\n",
    "            # add the data straight into the wavelet_features array\n",
    "            std = feature_append(std, np.std(array,1))\n",
    "        if any(i in ['mean_abs','ratio'] for i in feature_list):\n",
    "            # add the data into the mean_abs array\n",
    "            mean_abs = feature_append(mean_abs, np.mean(np.absolute(array),1))\n",
    "        if 'LSWT' in feature_list:\n",
    "            # add the data into the LSWT array\n",
    "            LSWT = feature_append(LSWT, np.sum(array,1))\n",
    "    \n",
    "    wavelet_features = np.array([])\n",
    "    if 'mean' in feature_list:\n",
    "        if scale:\n",
    "            mean = frequency_scale(mean)\n",
    "        # add the data straight into the wavelet_features array\n",
    "        wavelet_features = feature_append(wavelet_features, mean, expand=False)\n",
    "        # add the feature names\n",
    "        if channel_name:\n",
    "            feature_names.extend([channel_name+'|'+name+'_mean' for name in names])\n",
    "    \n",
    "    if 'std' in feature_list:\n",
    "        if scale:\n",
    "            std = frequency_scale(std)\n",
    "        # add the data straight into the wavelet_features array\n",
    "        wavelet_features = feature_append(wavelet_features, std, expand=False)\n",
    "        # add the feature names\n",
    "        if channel_name:\n",
    "            feature_names.extend([channel_name+'|'+name+'_std' for name in names])\n",
    "    \n",
    "    if 'ratio' in feature_list:\n",
    "        # make an empty df we will put data in\n",
    "        ratio = np.empty((mean_abs.shape))\n",
    "        # for each decomposition level\n",
    "        for level in range(0, mean_abs.shape[1]):\n",
    "            # for the first level\n",
    "            if level == 0:\n",
    "                ratio[:,level] = mean_abs[:,level]/mean_abs[:,level+1]\n",
    "            # for the last level\n",
    "            elif level == mean_abs.shape[1]-1:\n",
    "                ratio[:,level] = mean_abs[:,level]/mean_abs[:,level-1]\n",
    "            # all other levels\n",
    "            else:\n",
    "                mean_levels = (mean_abs[:,level-1]+mean_abs[:,level+1])/2\n",
    "                ratio[:,level] = mean_abs[:,level]/mean_levels\n",
    "        \n",
    "        if scale:\n",
    "            ratio = frequency_scale(ratio)\n",
    "        # concat the ratio\n",
    "        wavelet_features = feature_append(wavelet_features, ratio, expand=False)\n",
    "        if channel_name:\n",
    "            # add to the feature names\n",
    "            feature_names.extend([channel_name+'|'+name+'_ratio' for name in names])\n",
    "            \n",
    "    if 'mean_abs' in feature_list:\n",
    "        if scale:\n",
    "            mean_abs = frequency_scale(mean_abs)\n",
    "        # now add in the mean_abs to the feature list\n",
    "        wavelet_features = feature_append(wavelet_features, mean_abs, expand=False)\n",
    "        if channel_name:\n",
    "            # add to the feature names\n",
    "            feature_names.extend([channel_name+'|'+name+'_mean_abs' for name in names])\n",
    "    \n",
    "    if 'LSWT' in feature_list:        \n",
    "        # minus the smallest value from each level for each time\n",
    "        LSWT = LSWT.T - np.amin(LSWT,1)\n",
    "        # transpose back\n",
    "        LSWT = LSWT.T\n",
    "        # plus 1 to each datapoint\n",
    "        LSWT = LSWT+1\n",
    "        # log each level for each time\n",
    "        LSWT = np.log(LSWT)\n",
    "        if scale:\n",
    "            LSWT = frequency_scale(LSWT)\n",
    "        # append the feature onto the wavelet features\n",
    "        wavelet_features = feature_append(wavelet_features, LSWT, expand=False)\n",
    "        if channel_name:\n",
    "            # add to the feature names\n",
    "            feature_names.extend([channel_name+'|'+name+'_LSWT' for name in names])\n",
    "    \n",
    "    if channel_name:\n",
    "        return wavelet_features, feature_names\n",
    "    \n",
    "    else:\n",
    "        return wavelet_features\n",
    "    \n",
    "    \n",
    "def fft(time_data, fft_band):\n",
    "    def replaceZeroes(data):\n",
    "        min_nonzero = np.min(data[np.nonzero(data)])\n",
    "        data[data == 0] = min_nonzero\n",
    "        return data\n",
    "    ab_fft = np.absolute(np.fft.rfft(time_data, axis=1)[:,fft_band[0]:fft_band[1]])\n",
    "    ab_fft = replaceZeroes(ab_fft)\n",
    "    return np.log10(ab_fft)\n",
    "\n",
    "\n",
    "def correlation_matrix(data):\n",
    "    # in the rare case that there is an inf\n",
    "    # that came from the fft, turn it to a large number\n",
    "    #data[data == -inf] = np.nan\n",
    "    # Create the Scaler object\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    # scale data across each frequency\n",
    "    scaled = scaler.fit_transform(data)\n",
    "    # get the correlation coefficients from a Pearson product-moment (ignore nans)\n",
    "    return np.ma.corrcoef(scaled)\n",
    "\n",
    "\n",
    "# We get the eigenvalues and only take the first array out.\n",
    "# We get the absolute to make them 'real'\n",
    "def eigen(corr_matrix):\n",
    "    # in the rare case that there is an inf or nan\n",
    "    #corr_matrix = np.nan_to_num(corr_matrix)\n",
    "    eigen_data = np.absolute(np.linalg.eig(corr_matrix)[0])\n",
    "    # expand and transpose so it becomes columns\n",
    "    eigen_data = np.expand_dims(eigen_data, axis=1).T\n",
    "    \n",
    "    return eigen_data\n",
    "\n",
    "# essentially upper_right_triangle from MichaelHills\n",
    "def corr_reshape(matrix):\n",
    "    accum = []\n",
    "    for i in range(matrix.shape[0]):\n",
    "        # the +1 excludes a channels correlations with itself\n",
    "        for j in range(i+1, matrix.shape[1]):\n",
    "            accum.append(matrix[i, j])\n",
    "            \n",
    "    return np.expand_dims(np.array(accum), axis=0)\n",
    "\n",
    "def entropy(data, feature_list, sf, channel_name=None):\n",
    "    from entropy.entropy import sample_entropy, spectral_entropy\n",
    "    entropy_features = np.array([])\n",
    "    feature_names = []\n",
    "    \n",
    "    # change nans to 0's\n",
    "    data = np.nan_to_num(data)\n",
    "    \n",
    "    if 'sample_entropy' in feature_list:\n",
    "        sam_data = np.apply_along_axis(sample_entropy, 1, data)\n",
    "        sam_data[sam_data == 0] = np.nan\n",
    "        # now add in the data to the features\n",
    "        entropy_features = feature_append(entropy_features, sam_data, expand=True)\n",
    "        if channel_name:\n",
    "            # add to the feature names\n",
    "            feature_names.append(channel_name+'|sample_entropy')\n",
    "    \n",
    "    if 'spectral_entropy' in feature_list:\n",
    "        spec_data = np.apply_along_axis(spectral_entropy, 1, data, sf,\n",
    "                                        method='welch', nperseg = sf)\n",
    "        # now add in the data to the features\n",
    "        entropy_features = feature_append(entropy_features, spec_data, expand=True)\n",
    "        if channel_name:\n",
    "            # add to the feature names\n",
    "            feature_names.append(channel_name+'|spec_entropy')\n",
    "            \n",
    "    if channel_name:\n",
    "        return entropy_features, feature_names\n",
    "    \n",
    "    else:\n",
    "        return entropy_features\n",
    "    \n",
    "    \n",
    "def frequency_scale(data):\n",
    "    SS = StandardScaler()\n",
    "    orig_shape = data.shape\n",
    "    # shape the data into one row\n",
    "    data = data.reshape(-1, 1)\n",
    "    # scale data in respect to all frequencies\n",
    "    scaled_data = SS.fit_transform(data)\n",
    "    # shape the data back to before\n",
    "    scaled_data = scaled_data.reshape(orig_shape)\n",
    "    \n",
    "    return scaled_data\n",
    "'''\n",
    "=======================\n",
    "CLASS: Seizure_Features\n",
    "=======================\n",
    "\n",
    "- sf\n",
    "    - Sampling frequency\n",
    "- downsample\n",
    "    - Factor to downsample by\n",
    "- window_size\n",
    "    - Seconds(int)/datapoints(float) to epoch the data into\n",
    "    - Can be None for no epoching\n",
    "- overlap\n",
    "    - Seconds(int)/datapoints(float) overlap between windows\n",
    "    - Default None for no overlap\n",
    "- weighted\n",
    "    - If to apply a weighting to the window (default False)\n",
    "- feature_list\n",
    "    - list of features to be extracted\n",
    "        - power:\n",
    "        - power_ratio:\n",
    "        - mean:\n",
    "        - mean_abs:\n",
    "        - std:\n",
    "        - ratio:\n",
    "        - LSWT:\n",
    "        - fft_eigen:\n",
    "        - fft_corr:\n",
    "        - time_corr:\n",
    "        - time_eigen:\n",
    "        - sample_entropy: LIMITED IMPLIMENTATION\n",
    "        - spectral_entropy: LIMITED IMPLIMENTATION\n",
    "        - wavelet_coherence: NOT YET IMPLIMENTED\n",
    "- bandpasses\n",
    "    - list of bandpasses to extract for the power measure\n",
    "- bandpass_mean\n",
    "    - whether to take the mean or median of the Welch output\n",
    "- bandpass_ratios\n",
    "    - list of bandpasses to get a ratio between\n",
    "- wavelet\n",
    "    - type of wavelet to use\n",
    "- wavelet_transform\n",
    "    - type of transformation to use ('DWT' or 'UDWT')\n",
    "- levels\n",
    "    - how many levels to get from the wavelet transform\n",
    "- fft_band\n",
    "    - The fft band used for the fft_corr and fft_eigen methods\n",
    "- scale\n",
    "    - Whether to scale the data according to the mean so it has a standard deviation of 1.\n",
    "    - Features based on frequency will be scaled in respect to each other.\n",
    "    - If scikitlearn is >= 0.20.0 then you can leave NAN's in for the input if scaling\n",
    "- target\n",
    "    - the event target if doing binary classification\n",
    "    - will override baseline if both provided\n",
    "- baseline\n",
    "    - the event target representing the class of least interest\n",
    "    - if target and baseline both not provided then takes the most common class in window to classify window\n",
    "'''\n",
    "\n",
    "class Seizure_Features(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 sf,\n",
    "                 downsample=1,\n",
    "                 window_size=1,\n",
    "                 overlap=None,\n",
    "                 weighted=False,\n",
    "                 feature_list=['power', 'power_ratio', 'mean', 'mean_abs', 'std',\n",
    "                               'ratio', 'LSWT', 'fft_corr', 'fft_eigen',\n",
    "                               'time_corr', 'time_eigen', 'sample_entropy',\n",
    "                               'spectral_entropy'],\n",
    "                 bandpasses=[[1,4],[4,8]],\n",
    "                 bandpass_mean=False,\n",
    "                 bandpass_ratios=[[[3,12],[2,30]],],\n",
    "                 wavelet = 'db4',\n",
    "                 wavelet_transform = 'DWT',\n",
    "                 levels=6,\n",
    "                 fft_band=[1,48],\n",
    "                 scale = False,\n",
    "                 target=None, \n",
    "                 baseline=None\n",
    "                ):\n",
    "        self.sf = sf\n",
    "        self.downsample = downsample\n",
    "        if isinstance(window_size, int):\n",
    "            self.window_size = window_size*sf\n",
    "        else:\n",
    "            self.window_size = window_size\n",
    "        if isinstance(overlap, int):\n",
    "            self.overlap = overlap*sf\n",
    "        else:\n",
    "            self.overlap = overlap\n",
    "        self.weighted = weighted\n",
    "        self.feature_list = feature_list\n",
    "        self.bandpasses = bandpasses\n",
    "        self.bandpass_ratios = bandpass_ratios\n",
    "        self.bandpass_mean = bandpass_mean\n",
    "        self.wavelet=wavelet\n",
    "        self.wavelet_transform = wavelet_transform\n",
    "        self.levels=levels\n",
    "        self.fft_band = fft_band\n",
    "        self.target = target\n",
    "        self.baseline = baseline\n",
    "        self.scale = scale\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, channel_names_list=None):\n",
    "        # create empty arrays\n",
    "        feature_set = np.array([])\n",
    "        all_windowed_channels = np.array([])\n",
    "        # create empty list\n",
    "        feature_names = []\n",
    "        # specify types of features\n",
    "        wav_features = ['mean', 'mean_abs', 'std', 'ratio', 'LSWT']\n",
    "        eigen_corr_features = ['fft_corr', 'fft_eigen', 'time_corr', 'time_eigen']\n",
    "        entropy_features = ['app_entropy', 'spectral_entropy']\n",
    "        \n",
    "        # downsample\n",
    "        X = X[::self.downsample]\n",
    "        self.sf = self.sf/self.downsample\n",
    "        \n",
    "        # check if any of the rows have all the same number (maybe impedence testing?).\n",
    "        # This will throw off most of the feature extraction so turn them to nans\n",
    "        # Cant seem to think of a better way outside of pandas to do this!\n",
    "        # Also we need to make sure there is more than one channel before doing this!\n",
    "        if X.shape[1]>1:\n",
    "          X = pd.DataFrame(X)\n",
    "          all_same = X.eq(X.iloc[:, 0], axis=0).all(1)\n",
    "          X.loc[all_same] = np.nan\n",
    "          X = X.values\n",
    "    \n",
    "        # for each column of the data...\n",
    "        for j, column in enumerate(X.T):\n",
    "            # window the data so each row is another epoch\n",
    "            channel_windowed = window(column, w = self.window_size, o = self.overlap, copy = True)\n",
    "\n",
    "            # -----\n",
    "            # POWER\n",
    "            # -----\n",
    "            if 'power' in self.feature_list:\n",
    "                # create an empty array\n",
    "                welch_data = np.array([])\n",
    "                # for each bandpass in the bandpasses\n",
    "                for bandpass in self.bandpasses:\n",
    "                    # calculate the average or median of the frequency\n",
    "                    # band for all time points in the channel\n",
    "                    bandpass_data = bandpower(channel_windowed.T,\n",
    "                                              sf = self.sf,\n",
    "                                              weighted= self.weighted,\n",
    "                                              mean = self.bandpass_mean,\n",
    "                                              band = bandpass)\n",
    "                    \n",
    "                    welch_data = feature_append(welch_data, bandpass_data)\n",
    "\n",
    "                    # if channel_names were provided...\n",
    "                    if isinstance(channel_names_list, list):\n",
    "                        # ...append the channel and frequency band to the list\n",
    "                        feature_names.append(channel_names_list[j]+'|'+str(bandpass[0])+'_'+str(bandpass[1])+'Hz')\n",
    "                \n",
    "                if self.scale:\n",
    "                    welch_data = frequency_scale(welch_data)\n",
    "\n",
    "                # append the data straight into the feature set\n",
    "                feature_set = feature_append(feature_set, welch_data, expand=False)\n",
    "                \n",
    "\n",
    "            # BANDPASS RATIOS\n",
    "            if 'power_ratio' in self.feature_list:\n",
    "                for bandpass_ratio in self.bandpass_ratios:\n",
    "                    bandpass_1 = bandpower(channel_windowed.T,\n",
    "                                              sf = self.sf,\n",
    "                                              weighted= self.weighted,\n",
    "                                              mean = self.bandpass_mean,\n",
    "                                              band = bandpass_ratio[0])\n",
    "                    bandpass_2 = bandpower(channel_windowed.T,\n",
    "                                              sf = self.sf,\n",
    "                                              weighted= self.weighted,\n",
    "                                              mean = self.bandpass_mean,\n",
    "                                              band = bandpass_ratio[1])\n",
    "                    # divide bandpass 2 from 1\n",
    "                    relative_power = bandpass_2/bandpass_1\n",
    "                    \n",
    "                    if self.scale:\n",
    "                        relative_power = frequency_scale(relative_power)\n",
    "\n",
    "                    # append the data straight into the feature set\n",
    "                    feature_set = feature_append(feature_set, relative_power)\n",
    "                    # if channel_names were provided...\n",
    "                    if isinstance(channel_names_list, list):\n",
    "                        ratio_str = str(bandpass_ratio[0][0])+'_'+str(bandpass_ratio[0][1])+'/'+str(bandpass_ratio[1][0])+'_'+str(bandpass_ratio[1][1])+'Hz'\n",
    "                        # ...append the channel and frequency band to the list\n",
    "                        feature_names.append(channel_names_list[j]+'|Ratio_'+ratio_str)\n",
    "                        \n",
    "                        \n",
    "            # --------\n",
    "            # WAVELETS\n",
    "            # --------\n",
    "            if any(i in wav_features for i in self.feature_list):\n",
    "                # if channel_names were provided...\n",
    "                if isinstance(channel_names_list, list):\n",
    "                    # ... calculate all the requested wavelet features for the channel over\n",
    "                    # all the epochs\n",
    "                    wavelet_features, wavelet_feat_names = wavelet_decompose(channel_windowed,\n",
    "                                                                             self.feature_list,\n",
    "                                                                             channel_name=channel_names_list[j],\n",
    "                                                                             wavelet=self.wavelet,\n",
    "                                                                             wavelet_transform = self.wavelet_transform,\n",
    "                                                                             level=self.levels,\n",
    "                                                                             scale = self.scale)\n",
    "                    # extend the feature list with the wavelet feature list\n",
    "                    feature_names.extend(wavelet_feat_names)\n",
    "                else:\n",
    "                    # this is if we dont have the channel names\n",
    "                    wavelet_features = wavelet_decompose(channel_windowed,\n",
    "                                                         self.feature_list,\n",
    "                                                         wavelet=self.wavelet,\n",
    "                                                         wavelet_transform = self.wavelet_transform,\n",
    "                                                         level=self.levels,\n",
    "                                                         scale = self.scale)\n",
    "                    \n",
    "                # append the wavelet feature without expanding the data\n",
    "                feature_set = feature_append(feature_set, wavelet_features, expand=False)\n",
    "                \n",
    "            # -------\n",
    "            # Entropy\n",
    "            # -------\n",
    "            if 'sample_entropy' in self.feature_list or 'spectral_entropy' in self.feature_list:\n",
    "                if isinstance(channel_names_list, list):\n",
    "                    entropy_features, entropy_feat_names = entropy(channel_windowed,\n",
    "                                                                   self.feature_list,\n",
    "                                                                   self.sf,\n",
    "                                                                   channel_name=channel_names_list[j])\n",
    "                    # extend the feature list with the wavelet feature list\n",
    "                    feature_names.extend(entropy_feat_names)\n",
    "                else:\n",
    "                    entropy_features = entropy(channel_windowed, self.feature_list, self.sf)\n",
    "                    \n",
    "                if self.scale:\n",
    "                    SS = StandardScaler()\n",
    "                    # scale data for each feature separately\n",
    "                    entropy_features = SS.fit_transform(entropy_features)\n",
    "                # append the wavelet feature without expanding the data\n",
    "                feature_set = feature_append(feature_set, entropy_features, expand=False)\n",
    "             \n",
    "        # ----------\n",
    "        # EIGEN CORR\n",
    "        # ----------\n",
    "            # if any of the correlation or eigenvalue methods have been specified...\n",
    "            if any(i in eigen_corr_features for i in self.feature_list):\n",
    "                # append the window data\n",
    "                all_windowed_channels = feature_append(all_windowed_channels, channel_windowed, axis=2, expand=True)\n",
    "        \n",
    "        # if any of the correlation or eigenvalue methods have been specified...\n",
    "        if any(i in eigen_corr_features for i in self.feature_list):\n",
    "            # default bools so only need to check these rather than search\n",
    "            # through a list each epoch which i assume would take longer?\n",
    "            bool_dict = {'fft_eigen':False,\n",
    "                         'fft_corr': False,\n",
    "                         'time_eigen': False,\n",
    "                         'time_corr': False}\n",
    "            \n",
    "            if 'fft_corr' in self.feature_list:\n",
    "                bool_dict['fft_corr'] = True\n",
    "            if 'fft_eigen' in self.feature_list:\n",
    "                bool_dict['fft_eigen'] = True\n",
    "            if 'time_corr' in self.feature_list:\n",
    "                bool_dict['time_corr'] = True\n",
    "            if 'time_eigen' in self.feature_list:\n",
    "                bool_dict['time_eigen'] = True\n",
    "            \n",
    "            # create an empty array\n",
    "            all_eigen_corr = np.array([])\n",
    "            \n",
    "            # go across epochs so we have channels and a single epoch\n",
    "            # in the data\n",
    "            for index, epoch in enumerate(all_windowed_channels):\n",
    "                # create an empty array\n",
    "                epoch_eigen_corr = np.array([])\n",
    "                # for each key in the dictionary\n",
    "                for key in bool_dict.keys():\n",
    "                    # check it is activated\n",
    "                    if bool_dict[key]:\n",
    "                        # if there are any nans, inf\n",
    "                        # in the data then we will\n",
    "                        # just make this feature dataframe full of nan's so it doesnt crash\n",
    "                        if np.isnan(epoch).any() or np.isinf(epoch).any():\n",
    "                            if key in ['fft_corr','time_corr']:\n",
    "                                # get the length of all possible channel combinations plus channels with themselves\n",
    "                                len_combinations = len(list(combinations(range(epoch.shape[1]), 2)))\n",
    "                                eigen_corr_data = np.full((1,len_combinations), np.nan)\n",
    "                            else:    \n",
    "                                eigen_corr_data = np.full((1,epoch.shape[1]), np.nan)\n",
    "                        \n",
    "                        # if there are no nans\n",
    "                        else:\n",
    "                            if key in ['fft_corr','fft_eigen']:\n",
    "                                # get the fourier transform data\n",
    "                                fft_data = fft(epoch.T, self.fft_band)\n",
    "                                # get correlation matrix of channels over freq\n",
    "                                corr_matrix = correlation_matrix(fft_data)\n",
    "                            else:\n",
    "                                # get correlation matrix of channel over time\n",
    "                                corr_matrix = correlation_matrix(epoch.T)\n",
    "\n",
    "                            # for the eigen data\n",
    "                            if key in ['fft_eigen','time_eigen']:\n",
    "                                # get absolute eigenvalues\n",
    "                                eigen_corr_data = eigen(corr_matrix)\n",
    "                            \n",
    "                            # for the corr data\n",
    "                            else:\n",
    "                                eigen_corr_data = corr_reshape(corr_matrix)\n",
    "                                        \n",
    "                        # append the epoch feature without expanding the data\n",
    "                        epoch_eigen_corr = feature_append(epoch_eigen_corr, eigen_corr_data, axis=1, expand=False)\n",
    "\n",
    "                # append the feature without expanding the data\n",
    "                all_eigen_corr = feature_append(all_eigen_corr, epoch_eigen_corr, axis=0, expand=False)\n",
    "                        \n",
    "            if self.scale:\n",
    "                SS = StandardScaler()\n",
    "                # scale data for each feature separately\n",
    "                all_eigen_corr = SS.fit_transform(all_eigen_corr)\n",
    "            # append all the eigen_corr data to the main feature set\n",
    "            feature_set = feature_append(feature_set, all_eigen_corr, axis=1, expand=False)\n",
    "\n",
    "            # if channel_names were provided...\n",
    "            if isinstance(channel_names_list, list):\n",
    "                # append the feature names\n",
    "                if bool_dict['fft_eigen']:\n",
    "                    feature_names.extend([channel+'|fft_eigen' for channel in channel_names_list])\n",
    "                if bool_dict['time_eigen']:\n",
    "                    feature_names.extend([channel+'|time_eigen' for channel in channel_names_list])\n",
    "                if bool_dict['fft_corr'] or bool_dict['time_corr']:\n",
    "                    # get all combinations of channels\n",
    "                    combinations_list = list(combinations(channel_names_list, 2))\n",
    "                    # join the channels together\n",
    "                    corr_comb = ['_'.join(map(str,i)) for i in combinations_list]\n",
    "                    if bool_dict['fft_corr']:\n",
    "                        feature_names.extend([channel_comb+'|fft_corr' for channel_comb in corr_comb])\n",
    "                    if bool_dict['time_corr']:\n",
    "                        feature_names.extend([channel_comb+'|time_corr' for channel_comb in corr_comb])\n",
    "                    \n",
    "        # set feature names as a class attribute\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        # ------\n",
    "        # DATA Y\n",
    "        # ------\n",
    "        # if y is an array\n",
    "        if type(y).__module__ == np.__name__:\n",
    "            y = y[::self.downsample]\n",
    "            data_y = window_y(y, self.window_size, self.overlap, target=self.target, baseline=self.baseline)\n",
    "            \n",
    "            return feature_set, data_y\n",
    "        else:\n",
    "            return feature_set\n",
    "\n",
    "def exportFeatureDump(UPENN_SAVE_PATH, UPENN_DIR, part_id): \n",
    "    if os.path.exists(UPENN_SAVE_PATH):\n",
    "        os.remove(UPENN_SAVE_PATH)\n",
    "    part_file_list = file_list(os.path.join(UPENN_DIR, '*'), output=False)\n",
    "    for file in tqdm(part_file_list, desc=f'{part_id} Files'):\n",
    "        df, freq = mat_to_df(file)\n",
    "        class_name = file_class(file)\n",
    "    \n",
    "        feat = Seizure_Features(sf=freq,\n",
    "                                window_size=None,\n",
    "                                bandpasses=[[2, 4], [4, 8], [8, 12], [12, 30], [30, 70]],\n",
    "                                feature_list=['power', 'power_ratio', 'mean', 'mean_abs',\n",
    "                                              'std', 'ratio', 'LSWT', 'fft_corr', 'fft_eigen',\n",
    "                                              'time_corr', 'time_eigen'])\n",
    "    \n",
    "        part_x_feat = feat.transform(df.values, channel_names_list=list(df.columns))\n",
    "        part_x_feat = pd.DataFrame(part_x_feat, columns=feat.feature_names)\n",
    "    \n",
    "        part_y_feat = np.expand_dims(np.array([class_name]), axis=1)\n",
    "    \n",
    "        writeDumpFile(UPENN_SAVE_PATH, part_id, part_x_feat, part_y_feat)\n",
    "\n",
    "def writeDumpFile(save_dir, part_id, combined_df, condition):\n",
    "    h5file = tables.open_file(save_dir, mode=\"a\", title=\"Patient 2 Features\")\n",
    "\n",
    "    if \"/\" + part_id in h5file:\n",
    "        part_x_array = h5file.get_node(\"/\" + part_id + '/Data_x')\n",
    "        part_y_array = h5file.get_node(\"/\" + part_id + '/Data_y')\n",
    "\n",
    "        data_x_labels = h5file.get_node('/' + part_id + '/Data_x_Feat_Names')\n",
    "        combined_df = combined_df.reindex(data_x_labels[:].astype(str), axis=1)\n",
    "\n",
    "    else:\n",
    "        part_group = h5file.create_group(\"/\", part_id, 'Participant Data')\n",
    "        x_atom = tables.Atom.from_dtype(combined_df.values.dtype)\n",
    "        y_atom = tables.Atom.from_dtype(condition.dtype)\n",
    "\n",
    "        part_x_array = h5file.create_earray(\"/\" + part_id, 'Data_x', x_atom, (0, combined_df.shape[1]), 'Feature Array')\n",
    "        part_y_array = h5file.create_earray(\"/\" + part_id, 'Data_y', y_atom, (0, 1), 'Events Array')\n",
    "\n",
    "        h5file.create_array(\"/\" + part_id, 'Data_x_Feat_Names', np.array(combined_df.columns, dtype='unicode'), \"Names of Each Feature\")\n",
    "\n",
    "    part_x_array.append(combined_df.values)\n",
    "    part_y_array.append(condition)\n",
    "\n",
    "    h5file.flush()\n",
    "    h5file.close()\n",
    "\n",
    "\n",
    "def file_list(folder_path, output=False):\n",
    "    # create an empty list\n",
    "    file_list = []\n",
    "    # for file name in the folder path...\n",
    "    for filename in glob.glob(folder_path):\n",
    "        # ... append it to the list\n",
    "        file_list.append(filename)\n",
    "\n",
    "    # sort alphabetically\n",
    "    file_list.sort()\n",
    "\n",
    "    # Output\n",
    "    if output:\n",
    "        print(str(len(file_list)) + \" files found\")\n",
    "        pp.pprint(file_list)\n",
    "\n",
    "    return file_list\n",
    "    \n",
    "def file_class(file_name):\n",
    "    if re.findall('interictal', file_name):\n",
    "        return 0\n",
    "    elif re.findall('ictal', file_name):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def welchBandpower(data, sf, band, display_output=False):\n",
    "    \"\"\"\n",
    "    Calculate the band power of the given data using the Welch method.\n",
    "\n",
    "    Parameters:\n",
    "    - data (DataFrame): Input data containing the signals.\n",
    "    - sf (float): Sampling frequency of the data.\n",
    "    - band (list or array): Frequency band for which to calculate the power (e.g., [8, 12] for alpha band).\n",
    "    - display_output (bool): Whether to print the results (default is False).\n",
    "\n",
    "    Returns:\n",
    "    - Series: Mean power in the specified frequency band for each channel.\n",
    "    \"\"\"\n",
    "    # Convert all columns to numeric, coercing errors to NaN, and drop NaN values\n",
    "    data = data.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "    # Ensure band is an array\n",
    "    band = np.asarray(band)\n",
    "    low, high = band\n",
    "\n",
    "    # Calculate the appropriate nperseg\n",
    "    nperseg = min(len(data), int((2 / low) * sf))  # Ensure nperseg does not exceed the length of data\n",
    "\n",
    "    # Compute the periodogram using Welch's method\n",
    "    freqs, psd = welch(data.values,  # Pass data as numpy array\n",
    "                       sf,\n",
    "                       nperseg=nperseg,\n",
    "                       scaling='density',\n",
    "                       axis=0)\n",
    "\n",
    "    # Convert PSD to a DataFrame for easier manipulation\n",
    "    psd_df = pd.DataFrame(psd, index=freqs, columns=data.columns)\n",
    "\n",
    "    if display_output:\n",
    "        print(\"Welch Output:\")\n",
    "        psd_df.index.name = 'Frequency (Hz)'\n",
    "        psd_df.columns = ['Power']\n",
    "        display(psd_df)\n",
    "\n",
    "    # Find closest indices for the frequency band\n",
    "    idx_min = np.searchsorted(freqs, low)\n",
    "    idx_max = np.searchsorted(freqs, high)\n",
    "\n",
    "    # Ensure the indices are within bounds\n",
    "    idx_min = max(0, idx_min - 1)\n",
    "    idx_max = min(len(freqs) - 1, idx_max)\n",
    "\n",
    "    # Select frequencies of interest and compute mean power\n",
    "    band_psd = psd_df.iloc[idx_min:idx_max, :].mean()\n",
    "\n",
    "    if display_output:\n",
    "        print(\"\\nMean Frequency Band Power:\")\n",
    "        display(band_psd)\n",
    "\n",
    "    return band_psd\n",
    "\n",
    "\n",
    "def welchPowerMeasure(dataFrame, sampleRate):\n",
    "    \"\"\"\n",
    "    Calculate the mean power across specified frequency bands for given EEG data using the Welch method.\n",
    "\n",
    "    Parameters:\n",
    "    - dataFrame (DataFrame): A DataFrame containing EEG signal data with columns representing different channels.\n",
    "    - sampleRate (float): Sampling frequency of the data.\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame where each row corresponds to a frequency band and each column corresponds to a channel.\n",
    "                  The values represent the mean power in the specified frequency bands.\n",
    "\n",
    "    Frequency Bands:\n",
    "    - Delta: [0.1, 4] Hz\n",
    "    - Theta: [4, 8] Hz\n",
    "    - Alpha: [8, 12] Hz\n",
    "    - Beta: [12, 30] Hz\n",
    "    - Gamma: [30, 70] Hz\n",
    "\n",
    "    The function processes the input data by calculating the power spectral density (PSD) for each frequency band\n",
    "    using the Welch method. The mean power in each band is computed for all channels in the DataFrame.\n",
    "    \"\"\"\n",
    "    bandpasses = [[[0.1, 4], 'power_delta'],\n",
    "                  [[4, 8], 'power_theta'],\n",
    "                  [[8, 12], 'power_alpha'],\n",
    "                  [[12, 30], 'power_beta'],\n",
    "                  [[30, 70], 'power_gamma']]\n",
    "\n",
    "    welch_df = pd.DataFrame()\n",
    "    for bandpass, freq_name in bandpasses:\n",
    "        bandpass_data = welchBandpower(dataFrame, sampleRate, bandpass)\n",
    "        bandpass_data = pd.Series(bandpass_data, index=dataFrame.columns).rename(freq_name)\n",
    "\n",
    "        if welch_df.empty:\n",
    "            welch_df = pd.DataFrame(bandpass_data).T\n",
    "        else:\n",
    "            welch_df = pd.concat([welch_df, pd.DataFrame(bandpass_data).T], axis=0)\n",
    "\n",
    "    return welch_df\n",
    "\n",
    "\n",
    "def cumulateBandPower(data, fs):\n",
    "    \"\"\"\n",
    "    Calculate the cumulative power in specified EEG frequency bands for each channel.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pd.DataFrame\n",
    "        A DataFrame where each column represents an EEG channel.\n",
    "    - fs: float\n",
    "        The sampling frequency of the EEG data.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame\n",
    "        A DataFrame where each row represents the cumulative power for each frequency band \n",
    "        corresponding to the channels in the input data.\n",
    "        The columns are named in the format '{channel}_{band}', where 'band' is one of \n",
    "        'power_delta', 'power_theta', 'power_alpha', 'power_beta', 'power_gamma'.\n",
    "    \"\"\"\n",
    "    bands = {\n",
    "        'power_delta': (0.5, 4),\n",
    "        'power_theta': (4, 8),\n",
    "        'power_alpha': (8, 13),\n",
    "        'power_beta': (13, 30),\n",
    "        'power_gamma': (30, 100)\n",
    "    }\n",
    "    \n",
    "    band_powers = {f\"{col}_{band}\": [] for col in data.columns for band in bands}\n",
    "    nperseg = min(fs, len(data))  # Ensure nperseg does not exceed the input length\n",
    "\n",
    "    # Iterate over each column (EEG channel)\n",
    "    for col in data.columns:\n",
    "        freqs, psd = welch(data[col], fs, nperseg=nperseg)  # Use the adjusted nperseg\n",
    "        for band, (low, high) in bands.items():\n",
    "            # Extract the power in the frequency band\n",
    "            band_power = np.trapz(\n",
    "                psd[(freqs >= low) & (freqs < high)], \n",
    "                freqs[(freqs >= low) & (freqs < high)]\n",
    "            )\n",
    "            band_powers[f\"{col}_{band}\"].append(band_power)\n",
    "\n",
    "    # Combine into a DataFrame with each column representing a band power for a channel\n",
    "    band_power_df = pd.DataFrame(band_powers)\n",
    "    return band_power_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/24 [00:01<00:30,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb01: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/24 [00:02<00:27,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb02: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 3/24 [00:03<00:25,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb03: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 4/24 [00:04<00:24,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb04: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 5/24 [00:06<00:22,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb05: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 6/24 [00:07<00:21,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb06: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 7/24 [00:09<00:24,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb07: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 8/24 [00:10<00:22,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb08: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 9/24 [00:12<00:23,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb09: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 10/24 [00:13<00:20,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb10: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 11/24 [00:14<00:17,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb11: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 12/24 [00:16<00:16,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb12: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 13/24 [00:18<00:17,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb13: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 14/24 [00:19<00:14,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb14: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 15/24 [00:20<00:12,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb15: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 16/24 [00:22<00:13,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb16: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 17/24 [00:24<00:10,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb17: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 18/24 [00:25<00:08,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb18: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 19/24 [00:30<00:12,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb19: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 20/24 [00:31<00:08,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb20: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 21/24 [00:32<00:05,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb21: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 22/24 [00:34<00:03,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb22: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 23/24 [00:36<00:01,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb23: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:37<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading or reading summary file for chb24: [WinError 32] The process cannot access the file because it is being used by another process: './chbmit_summary.txt'\n",
      "All data downloaded and preprocessed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyedflib\n",
    "import tables\n",
    "from urllib.request import urlretrieve\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to get content from the summary file of a participant\n",
    "def get_content(part_code):\n",
    "    url = f\"https://physionet.org/content/chbmit/1.0.0/{part_code}/{part_code}-summary.txt\"\n",
    "    filename = \"./chbmit_summary.txt\"\n",
    "    try:\n",
    "        urlretrieve(url, filename)\n",
    "\n",
    "        # Read the file into a list\n",
    "        with open(filename, encoding='UTF-8') as f:\n",
    "            content = f.readlines()\n",
    "            os.remove(filename)\n",
    "\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading or reading summary file for {part_code}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to extract information from the summary content\n",
    "def info_dict(content):\n",
    "    line_nos = len(content)\n",
    "    line_no = 1\n",
    "\n",
    "    channels = []\n",
    "    file_name = []\n",
    "    file_info_dict = {}\n",
    "\n",
    "    for line in content:\n",
    "        if re.search(r'Channel \\d+', line):\n",
    "            channel = line.split(': ')[-1].strip()\n",
    "            channels.append(channel)\n",
    "\n",
    "        elif re.search(r'File Name', line):\n",
    "            if file_name:\n",
    "                part_info_dict[file_name] = file_info_dict\n",
    "\n",
    "            file_name = re.findall(r'\\w+\\d+_\\d+|\\w+\\d+\\w+_\\d+', line)[0]\n",
    "            file_info_dict = {}\n",
    "            file_info_dict['Channels'] = list(set(channels))\n",
    "            file_info_dict['Start Time'] = ''\n",
    "            file_info_dict['End Time'] = ''\n",
    "            file_info_dict['Seizures Window'] = []\n",
    "\n",
    "        elif re.search(r'File Start Time', line):\n",
    "            file_info_dict['Start Time'] = re.findall(r'\\d+:\\d+:\\d+', line)[0]\n",
    "\n",
    "        elif re.search(r'File End Time', line):\n",
    "            file_info_dict['End Time'] = re.findall(r'\\d+:\\d+:\\d+', line)[0]\n",
    "\n",
    "        elif re.search(r'Seizure Start Time|Seizure End Time|Seizure \\d+ Start Time|Seizure \\d+ End Time', line):\n",
    "            file_info_dict['Seizures Window'].append(int(re.findall(r'\\d+', line)[-1]))\n",
    "\n",
    "        if line_no == line_nos:\n",
    "            part_info_dict[file_name] = file_info_dict\n",
    "\n",
    "        line_no += 1\n",
    "\n",
    "# Function to load data from an EDF file\n",
    "def data_load(file, selected_channels=[]):\n",
    "    try:\n",
    "        url = f\"https://physionet.org/content/chbmit/1.0.0/{file}\"\n",
    "        filename = \"./chbmit.edf\"\n",
    "\n",
    "        urlretrieve(url, filename)\n",
    "        f = pyedflib.EdfReader(filename)\n",
    "        os.remove(filename)\n",
    "\n",
    "        if len(selected_channels) == 0:\n",
    "            selected_channels = f.getSignalLabels()\n",
    "\n",
    "        channel_names = f.getSignalLabels()\n",
    "        channel_freq = f.getSampleFrequencies()\n",
    "\n",
    "        sigbufs = np.zeros((f.getNSamples()[0], len(selected_channels)))\n",
    "        for i, channel in enumerate(selected_channels):\n",
    "            sigbufs[:, i] = f.readSignal(channel_names.index(channel))\n",
    "\n",
    "        df = pd.DataFrame(sigbufs, columns=selected_channels).astype('float32')\n",
    "        index_increase = np.linspace(0, len(df) / channel_freq[0], len(df), endpoint=False)\n",
    "        seconds = np.floor(index_increase).astype('uint16')\n",
    "        df['Time'] = seconds\n",
    "        df = df.set_index('Time')\n",
    "        df.columns.name = 'Channel'\n",
    "\n",
    "        return df, channel_freq[0]\n",
    "\n",
    "    except OSError as e:\n",
    "        print(f\"Error loading EDF file {file}: {e}\")\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "# Start processing\n",
    "part_info_dict = {}\n",
    "data_path = \"./clips/\"\n",
    "save_dir = \"./output/chbmit_preprocessed.h5\"\n",
    "\n",
    "# Ensure the destination directory exists\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# Download and process each participant\n",
    "participants = [f\"chb{str(i).zfill(2)}\" for i in range(1, 25)]\n",
    "for part_code in tqdm(participants):\n",
    "    content = get_content(part_code)\n",
    "    info_dict(content)\n",
    "\n",
    "    edf_files = [f\"{part_code}/{f}\" for f in os.listdir(data_path) if f.endswith('.edf')]\n",
    "\n",
    "    for part_file in edf_files:\n",
    "        df, freq = data_load(part_file)\n",
    "        if not df.empty and freq:\n",
    "            events = create_events(part_file, df)\n",
    "            if not events.empty:\n",
    "                # Process and save data (similar to the earlier logic)\n",
    "                pass\n",
    "\n",
    "print(\"All data downloaded and preprocessed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: ./chbmit_data/chb01/chb01-summary.txt, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_01.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_02.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_03.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_03.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_04.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_04.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_05.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_06.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_07.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_08.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_09.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_10.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_11.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_12.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_13.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_14.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_15.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_15.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_16.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_16.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_17.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_18.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_18.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_19.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_20.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_21.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_21.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_22.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_23.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_24.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_25.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_26.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_26.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_27.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_29.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_30.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_31.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_32.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_33.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_34.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_36.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_37.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_38.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_39.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_40.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_41.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_42.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_43.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb01/chb01_46.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02-summary.txt, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_01.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_02.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_03.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_04.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_05.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_06.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_07.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_08.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_09.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_10.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_11.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_12.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_13.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_14.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_15.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_16%2B.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_16%2B.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_16.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_16.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_17.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_18.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_19.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_19.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_20.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_21.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_22.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_23.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_24.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_25.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_26.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_27.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_28.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_29.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_30.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_31.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_32.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_33.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_34.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb02/chb02_35.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03-summary.txt, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_01.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_01.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_02.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_02.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_03.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_03.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_04.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_04.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_05.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_06.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_07.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_08.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_09.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_10.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_11.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_12.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_13.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_14.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_15.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_16.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_17.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_18.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_19.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_20.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_21.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_22.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_23.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_24.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_25.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_26.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_27.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_28.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_29.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_30.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_31.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_32.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_33.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_34.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_34.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_35.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_35.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_36.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_36.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_37.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb03/chb03_38.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04-summary.txt, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_01.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_02.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_03.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_04.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_05.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_05.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_06.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_07.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_08.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_08.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_09.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_10.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_11.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_12.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_13.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_14.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_15.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_16.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_17.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_18.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_19.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_21.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_22.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_23.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_24.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_25.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_26.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_27.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_28.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_28.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_29.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_30.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_31.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_32.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_33.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_34.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_35.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_36.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_37.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_38.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_39.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_40.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_41.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_42.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb04/chb04_43.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05-summary.txt, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_01.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_02.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_03.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_04.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_05.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_06.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_06.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_07.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_08.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_09.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_10.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_11.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_12.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_13.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_13.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_14.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_15.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_16.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_16.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_17.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_17.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_18.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_19.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_20.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_21.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_22.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_22.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_23.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_24.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_25.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_26.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_27.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_28.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_29.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_30.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_31.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_32.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_33.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_34.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_35.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_36.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_37.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_38.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb05/chb05_39.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06-summary.txt, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_01.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_01.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_02.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_03.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_04.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_04.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_05.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_06.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_07.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_08.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_09.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_09.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_10.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_10.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_12.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_13.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_13.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_14.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_15.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_16.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_17.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_18.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_18.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_24.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb06/chb06_24.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07-summary.txt, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_01.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_02.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_03.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_04.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_05.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_06.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_07.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_08.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_09.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_10.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_11.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_12.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_12.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_13.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_13.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_14.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_15.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_16.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_17.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_18.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_19.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb07/chb07_19.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08-summary.txt, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_02.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_02.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_03.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_04.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_05.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_05.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_10.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_11.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_11.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_12.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_13.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_13.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_14.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_15.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_16.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_17.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_18.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_19.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_20.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_21.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_21.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_22.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_23.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_24.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb08/chb08_29.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09-summary.txt, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_01.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_02.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_03.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_04.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_05.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_06.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_06.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_07.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_08.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_08.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_09.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_10.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_11.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_12.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_13.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_14.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_15.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_16.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_17.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_18.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_19.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb09/chb09_19.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10-summary.txt, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_01.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_02.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_03.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_04.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_05.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_06.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_07.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_08.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_12.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_12.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_13.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_14.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_15.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_16.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_17.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_18.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_19.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_20.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_20.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_21.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_22.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_27.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_27.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_28.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_30.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_30.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_31.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_31.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_38.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_38.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_89.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb10/chb10_89.edf.seizures, skipping download.\n",
      "File already exists: ./chbmit_data/chb11/chb11-summary.txt, skipping download.\n",
      "File already exists: ./chbmit_data/chb11/chb11_01.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb11/chb11_02.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb11/chb11_03.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb11/chb11_04.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb11/chb11_05.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb11/chb11_06.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb11/chb11_07.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb11/chb11_08.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb11/chb11_09.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb11/chb11_10.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb11/chb11_11.edf, skipping download.\n",
      "File already exists: ./chbmit_data/chb11/chb11_12.edf, skipping download.\n",
      "Downloaded: ./chbmit_data/chb11/chb11_13.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_14.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_15.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_16.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_17.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_18.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_19.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_24.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_25.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_26.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_27.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_53.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_54.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_55.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_56.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_58.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_60.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_61.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_62.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_63.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_82.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_82.edf.seizures\n",
      "Downloaded: ./chbmit_data/chb11/chb11_92.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_92.edf.seizures\n",
      "Downloaded: ./chbmit_data/chb11/chb11_99.edf\n",
      "Downloaded: ./chbmit_data/chb11/chb11_99.edf.seizures\n",
      "Downloaded: ./chbmit_data/chb12/chb12-summary.txt\n",
      "Downloaded: ./chbmit_data/chb12/chb12_06.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_06.edf.seizures\n",
      "Downloaded: ./chbmit_data/chb12/chb12_08.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_08.edf.seizures\n",
      "Downloaded: ./chbmit_data/chb12/chb12_09.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_09.edf.seizures\n",
      "Downloaded: ./chbmit_data/chb12/chb12_10.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_10.edf.seizures\n",
      "Downloaded: ./chbmit_data/chb12/chb12_11.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_11.edf.seizures\n",
      "Downloaded: ./chbmit_data/chb12/chb12_19.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_20.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_21.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_23.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_23.edf.seizures\n",
      "Downloaded: ./chbmit_data/chb12/chb12_24.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_27.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_27.edf.seizures\n",
      "Downloaded: ./chbmit_data/chb12/chb12_28.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_28.edf.seizures\n",
      "Downloaded: ./chbmit_data/chb12/chb12_29.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_29.edf.seizures\n",
      "Downloaded: ./chbmit_data/chb12/chb12_32.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_33.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_33.edf.seizures\n",
      "Downloaded: ./chbmit_data/chb12/chb12_34.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_35.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_36.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_36.edf.seizures\n",
      "Downloaded: ./chbmit_data/chb12/chb12_37.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_38.edf\n",
      "Downloaded: ./chbmit_data/chb12/chb12_38.edf.seizures\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Base URL of the dataset\n",
    "BASE_URL = \"https://physionet.org/files/chbmit/1.0.0/\"\n",
    "\n",
    "# Local directory to save the dataset\n",
    "SAVE_DIR = \"./chbmit_data/\"\n",
    "\n",
    "# Ensure the save directory exists\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    \"\"\"Download a single file from a URL.\"\"\"\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"File already exists: {save_path}, skipping download.\")\n",
    "        return\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an error for HTTP issues\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "def crawl_and_download(base_url, save_dir):\n",
    "    \"\"\"Crawl and download all files from a given base URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Loop through all links on the webpage\n",
    "        for link in soup.find_all(\"a\"):\n",
    "            href = link.get(\"href\")\n",
    "            if not href or href in [\"../\"]:\n",
    "                continue  # Skip parent directory links\n",
    "            \n",
    "            # Form the full URL\n",
    "            full_url = urljoin(base_url, href)\n",
    "            # Form the local save path\n",
    "            local_path = os.path.join(save_dir, href)\n",
    "\n",
    "            # If the link points to a directory, recurse\n",
    "            if href.endswith(\"/\"):\n",
    "                os.makedirs(local_path, exist_ok=True)\n",
    "                crawl_and_download(full_url, local_path)\n",
    "            else:\n",
    "                # Download the file\n",
    "                download_file(full_url, local_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error crawling {base_url}: {e}\")\n",
    "\n",
    "# Start the recursive download\n",
    "crawl_and_download(BASE_URL, SAVE_DIR)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "eeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
