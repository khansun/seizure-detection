{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gm9X27QALJzE"
   },
   "source": [
    "# *Pre-processing and Feature Extraction*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utility Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.preProcessor import *\n",
    "from utility.featureExtractor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "zL_7XiitWu9h",
    "outputId": "c77ffc4c-748b-4e16-996d-de2a0db01b38"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def find_files(url, headers):\n",
    "    # Access the directory URL\n",
    "    response = requests.get(url, auth=(headers['user'], headers['passwd']))\n",
    "    soup = BeautifulSoup(response.text, features=\"html.parser\")\n",
    "    \n",
    "    # Separate files and directories\n",
    "    hrefs_files = []\n",
    "    hrefs_dirs = []\n",
    "    \n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if href and not href.startswith('.'):\n",
    "            if href.endswith('/'):\n",
    "                hrefs_dirs.append(href.strip('/'))\n",
    "            else:\n",
    "                hrefs_files.append(href)\n",
    "    return hrefs_files, hrefs_dirs\n",
    "\n",
    "def download_file(download_file_url, file_path, headers, output=False):\n",
    "    if output:\n",
    "        print('Downloading:', download_file_url)\n",
    "    r = requests.get(download_file_url, auth=(headers['user'], headers['passwd']))\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "def download_TUH(DOWNLOAD_DIR, headers, sub_dir='', output=False):\n",
    "    # Base URL for the dataset\n",
    "    base_url = 'https://isip.piconepress.com/projects/nedc/data/tuh_eeg/tuh_eeg_seizure/v2.0.3/edf/'\n",
    "    dir_url = urljoin(base_url, sub_dir)\n",
    "    \n",
    "    # Clean up export_dir path for local storage\n",
    "    export_dir = os.path.join(DOWNLOAD_DIR, re.sub(r'.*edf/', '', sub_dir))\n",
    "    \n",
    "    if not os.path.exists(export_dir):\n",
    "        os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    # Get lists of files and directories\n",
    "    files, dirs = find_files(dir_url, headers)\n",
    "    \n",
    "    # Download all files in the current directory\n",
    "    for file in files:\n",
    "        if re.search(r'\\.xlsx$|\\.edf$|\\.txt$|\\.tse(?!_)', file):\n",
    "            file_path = os.path.join(export_dir, file)\n",
    "            if not os.path.exists(file_path):\n",
    "                download_file(urljoin(dir_url, file), file_path, headers, output)\n",
    "\n",
    "    # Recursively process each subdirectory\n",
    "    for subfolder in dirs:\n",
    "        next_sub_dir = os.path.join(sub_dir, subfolder)\n",
    "        download_TUH(DOWNLOAD_DIR, headers, next_sub_dir, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPSConnectionPool(host='isip.piconepress.com', port=443): Max retries exceeded with url: /projects/nedc/data/tuh_eeg/tuh_eeg_seizure/v2.0.3/edf/train%5Caaaaastr (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000025F7718FA40>, 'Connection to isip.piconepress.com timed out. (connect timeout=None)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    692\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:208\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    211\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x0000025F7718FA40>, 'Connection to isip.piconepress.com timed out. (connect timeout=None)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='isip.piconepress.com', port=443): Max retries exceeded with url: /projects/nedc/data/tuh_eeg/tuh_eeg_seizure/v2.0.3/edf/train%5Caaaaastr (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000025F7718FA40>, 'Connection to isip.piconepress.com timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRLYF8ZhBMZwNnsYA8FsP\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m auth_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m: user, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpasswd\u001b[39m\u001b[38;5;124m'\u001b[39m: key}\n\u001b[1;32m---> 22\u001b[0m \u001b[43mdownload_TUH\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDOWNLOAD_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 56\u001b[0m, in \u001b[0;36mdownload_TUH\u001b[1;34m(DOWNLOAD_DIR, headers, sub_dir, output)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subfolder \u001b[38;5;129;01min\u001b[39;00m dirs:\n\u001b[0;32m     55\u001b[0m     next_sub_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sub_dir, subfolder)\n\u001b[1;32m---> 56\u001b[0m     \u001b[43mdownload_TUH\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDOWNLOAD_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_sub_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 56\u001b[0m, in \u001b[0;36mdownload_TUH\u001b[1;34m(DOWNLOAD_DIR, headers, sub_dir, output)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subfolder \u001b[38;5;129;01min\u001b[39;00m dirs:\n\u001b[0;32m     55\u001b[0m     next_sub_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sub_dir, subfolder)\n\u001b[1;32m---> 56\u001b[0m     \u001b[43mdownload_TUH\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDOWNLOAD_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_sub_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 44\u001b[0m, in \u001b[0;36mdownload_TUH\u001b[1;34m(DOWNLOAD_DIR, headers, sub_dir, output)\u001b[0m\n\u001b[0;32m     41\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(export_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Get lists of files and directories\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m files, dirs \u001b[38;5;241m=\u001b[39m \u001b[43mfind_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Download all files in the current directory\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m, in \u001b[0;36mfind_files\u001b[1;34m(url, headers)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_files\u001b[39m(url, headers):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Access the directory URL\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpasswd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Separate files and directories\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py:688\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[1;32m--> 688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectTimeout\u001b[0m: HTTPSConnectionPool(host='isip.piconepress.com', port=443): Max retries exceeded with url: /projects/nedc/data/tuh_eeg/tuh_eeg_seizure/v2.0.3/edf/train%5Caaaaastr (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000025F7718FA40>, 'Connection to isip.piconepress.com timed out. (connect timeout=None)'))"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "import sys\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "\n",
    "DOWNLOAD_DIR = os.path.expanduser('tuh_data')  # Set a local path\n",
    "\n",
    "if not os.path.exists(DOWNLOAD_DIR):\n",
    "  os.makedirs(DOWNLOAD_DIR)\n",
    "\n",
    "user = \"nedc-tuh-eeg\"\n",
    "key = \"RLYF8ZhBMZwNnsYA8FsP\"\n",
    "\n",
    "auth_dict = {'user': user, 'passwd': key}\n",
    "\n",
    "download_TUH(DOWNLOAD_DIR, auth_dict, '', output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Process and Export Feature Dump*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUH_FILT_OVERWRITE = True  # or False, depending on your desired behavior\n",
    "TUH_FEAT_OVERWRITE = True  # or False, depending on your desired behavior\n",
    "TUH_UDWT_OVERWRITE = True  # or False, depending on your desired behavior\n",
    "TUH_FILT_SAVE_PATH = 'filtered_data.h5'  # Replace with your desired file path\n",
    "TUH_FEAT_SAVE_PATH = 'features_data.h5'  # Replace with your desired file path\n",
    "TUH_UDWT_SAVE_PATH = 'udwt_data.h5'  # Replace with your desired file path\n",
    "TUH_code = 'fnsz' # or another seizure type if you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:48: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:48: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_10568\\677058608.py:48: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  if re.findall('.xlsx|\\.edf|\\.tse(?!_)', str(link)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Worksheet named 'train' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 338\u001b[0m\n\u001b[0;32m    335\u001b[0m int_code \u001b[38;5;241m=\u001b[39m { k\u001b[38;5;241m.\u001b[39mlower() : v \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m int_code\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, \u001b[38;5;28mfloat\u001b[39m)}\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# get a list of files\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m tuh_file_list \u001b[38;5;241m=\u001b[39m \u001b[43msel_file_list\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTUH_code\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39msel_file_list(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev_test\u001b[39m\u001b[38;5;124m'\u001b[39m, TUH_code)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;66;03m# get a list of the montages\u001b[39;00m\n\u001b[0;32m    341\u001b[0m montage \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[9], line 189\u001b[0m, in \u001b[0;36msel_file_list\u001b[1;34m(set_name, seiz_type)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msel_file_list\u001b[39m(set_name, seiz_type):\n\u001b[0;32m    188\u001b[0m   \u001b[38;5;66;03m# load the training information\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m   train_info \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset/tuh_data/DOCS/seizures_types_v02.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m   \u001b[38;5;66;03m# just want the info per file here\u001b[39;00m\n\u001b[0;32m    191\u001b[0m   file_info \u001b[38;5;241m=\u001b[39m train_info\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m6101\u001b[39m,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m15\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:508\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 508\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1616\u001b[0m, in \u001b[0;36mExcelFile.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\n\u001b[0;32m   1577\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1578\u001b[0m     sheet_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m   1597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m \u001b[38;5;124;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1614\u001b[0m \u001b[38;5;124;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[0;32m   1615\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:773\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading sheet \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masheetname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(asheetname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 773\u001b[0m     sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sheet_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43masheetname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# assume an integer if not a string\u001b[39;00m\n\u001b[0;32m    775\u001b[0m     sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sheet_by_index(asheetname)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:582\u001b[0m, in \u001b[0;36mOpenpyxlReader.get_sheet_by_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sheet_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 582\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_if_bad_sheet_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbook[name]\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:624\u001b[0m, in \u001b[0;36mBaseExcelReader.raise_if_bad_sheet_by_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_if_bad_sheet_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msheet_names:\n\u001b[1;32m--> 624\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorksheet named \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Worksheet named 'train' not found"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import pyedflib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import tables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "# Import the getpass module\n",
    "from getpass import getpass\n",
    "\n",
    "def find_files(url, headers):\n",
    "    # get a soup of the directory url\n",
    "    soup = BeautifulSoup(requests.get(url, auth=(headers['user'], headers['passwd'])).text, \n",
    "                         features=\"html.parser\")\n",
    "    # make a list of all the links in the url\n",
    "    hrefs_list = []\n",
    "    for link in soup.find_all('a'):\n",
    "        hrefs_list.append(link.get('href'))\n",
    "\n",
    "    return hrefs_list\n",
    "    \n",
    "    \n",
    "def download_file(download_file_url, file_path, headers, output=False):\n",
    "    if output:\n",
    "        # print it is downloading\n",
    "        print('Downloading: '+ download_file_url)\n",
    "    # download the file to the directory\n",
    "    r = requests.get(download_file_url, auth=(headers['user'], headers['passwd']))\n",
    "    with open(file_path, 'wb') as f:\n",
    "      f.write(r.content)\n",
    "\n",
    "# needs a directory to download it to\n",
    "def download_TUH(DIR, headers, sub_dir, output=False):\n",
    "    \n",
    "    # directory url\n",
    "    dir_url = 'https://www.isip.piconepress.com/projects/tuh_eeg/downloads/tuh_eeg_seizure/v1.5.0/'+sub_dir\n",
    "\n",
    "    hrefs_dir_list = find_files(dir_url, headers)\n",
    "    \n",
    "    # for each link in the directory\n",
    "    for link in hrefs_dir_list:\n",
    "        # download the files outside of participant folders we want\n",
    "        if re.findall('.xlsx|\\.edf|\\.tse(?!_)', str(link)):\n",
    "            # if the file doesnt already exist in the directory\n",
    "            if not os.path.exists(os.path.join(DIR, link)):\n",
    "                download_file(dir_url+'/'+str(link), DIR+'/'+str(link), headers, output)\n",
    "\n",
    "def data_load(data_file, selected_channels=[]):\n",
    "\n",
    "    try:\n",
    "        # use the reader to get an EdfReader file\n",
    "        f = pyedflib.EdfReader(data_file)\n",
    "\n",
    "        # get the names of the signals\n",
    "        channel_names = f.getSignalLabels()\n",
    "        # get the sampling frequencies of each signal\n",
    "        channel_freq = f.getSampleFrequencies()\n",
    "        \n",
    "        # get a list of the EEG channels\n",
    "        if len(selected_channels) == 0:\n",
    "            selected_channels = channel_names\n",
    "\n",
    "        # make an empty file of 0's\n",
    "        sigbufs = np.zeros((f.getNSamples()[0],len(selected_channels)))\n",
    "        # for each of the channels in the selected channels\n",
    "        for i, channel in enumerate(selected_channels):\n",
    "        \n",
    "            try:\n",
    "              # add the channel data into the array\n",
    "              sigbufs[:, i] = f.readSignal(channel_names.index(channel))\n",
    "            \n",
    "            except:\n",
    "              ValueError\n",
    "              # This happens if the sampling rate of that channel is \n",
    "              # different to the others.\n",
    "              # For simplicity, in this case we just make it na.\n",
    "              sigbufs[:, i] = np.nan\n",
    "\n",
    "        # turn to a pandas df and save a little space\n",
    "        df = pd.DataFrame(sigbufs, columns = selected_channels).astype('float32')\n",
    "\n",
    "        # get equally increasing numbers upto the length of the data depending\n",
    "        # on the length of the data divided by the sampling frequency\n",
    "        index_increase = np.linspace(0,\n",
    "                                      len(df)/channel_freq[0],\n",
    "                                      len(df), endpoint=False)\n",
    "\n",
    "        # round these to the lowest nearest decimal to get the seconds\n",
    "        seconds = np.floor(index_increase).astype('uint16')\n",
    "\n",
    "        seconds = index_increase\n",
    "        \n",
    "        # make a column the timestamp\n",
    "        df['Time'] = seconds\n",
    "\n",
    "        # make the time stamp the index\n",
    "        df = df.set_index('Time')\n",
    "\n",
    "        # name the columns as channel\n",
    "        df.columns.name = 'Channel'\n",
    "\n",
    "        return df, channel_freq[0]\n",
    "\n",
    "    except OSError as error:\n",
    "        print('Error '+data_file+': '+str(error))\n",
    "        return pd.DataFrame(), None\n",
    "    except ValueError as error:\n",
    "        print('Error '+data_file+': '+str(error))\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "def create_events(file_name, df, code = None):\n",
    "\n",
    "    data_y = pd.Series(index=df.index)\n",
    "    data_y.name = 'Events'\n",
    "\n",
    "    events_tse = pd.read_csv(file_name, \n",
    "                             skiprows=1,\n",
    "                             sep = ' ',\n",
    "                             header=None,\n",
    "                             names =['Start', 'End', 'Code', 'Certainty'])\n",
    "    \n",
    "    data_y = data_y.fillna('bckg')\n",
    "    \n",
    "    for pos, row in events_tse.iterrows():\n",
    "        # if you want to manually set the code\n",
    "        if code != None:\n",
    "          if row['Code'] == code:\n",
    "              data_y[row['Start']:row['End']] = code\n",
    "        # let it be the code it is in the event file\n",
    "        else:\n",
    "          data_y[row['Start']:row['End']] = row['Code']\n",
    "\n",
    "    return data_y\n",
    "\n",
    "def window_y(events, window_size, overlap, target=None, baseline=None):\n",
    "    \n",
    "  # window the data so each row is another epoch\n",
    "  events_windowed = window(events, w = window_size, o = overlap, copy = True)\n",
    "  \n",
    "  if target:\n",
    "    # turn to array of bools if seizure in the\n",
    "    # windowed data\n",
    "    bools = events_windowed == target\n",
    "    # are there any seizure seconds in the data?\n",
    "    data_y = np.any(bools,axis=1)\n",
    "    # turn to 0's and 1's\n",
    "    data_y = data_y.astype(int)\n",
    "    # expand the dimensions so running down one column\n",
    "    data_y = np.expand_dims(data_y, axis=1)\n",
    "  \n",
    "  elif baseline:\n",
    "    # replace all baseline labels to nan\n",
    "    data_y = pd.DataFrame(events_windowed).replace(baseline, np.nan)\n",
    "    # get the most common other than baseline\n",
    "    data_y = data_y.mode(1)\n",
    "    # change nan back to baseline class\n",
    "    data_y = data_y.fillna(baseline).values\n",
    "    # if there was nothing but baseline there will be an empty array\n",
    "    if data_y.size == 0:\n",
    "        data_y = np.array([baseline]*data_y.shape[0])\n",
    "        data_y = np.expand_dims(data_y, -1)\n",
    "  \n",
    "  else:\n",
    "    # get the value most frequent in the window\n",
    "    data_y = pd.DataFrame(events_windowed).mode(1).values\n",
    "\n",
    "  return data_y\n",
    "\n",
    "def downsample(data_x, data_y, freq):\n",
    "    if freq > 256:\n",
    "        if freq >= 1000:\n",
    "          subsample = 4\n",
    "        else:\n",
    "          subsample = 2\n",
    "\n",
    "        freq = freq/subsample\n",
    "        data_x = data_x[::subsample]\n",
    "        data_y = data_y[::subsample]\n",
    "\n",
    "    return data_x, data_y, freq\n",
    "\n",
    "def sel_file_list(set_name, seiz_type):\n",
    "  # load the training information\n",
    "  train_info = pd.read_excel('dataset/tuh_data/DOCS/seizures_types_v02.xlsx', set_name)\n",
    "  # just want the info per file here\n",
    "  file_info = train_info.iloc[1:6101,1:15]\n",
    "  # cleans some of the names\n",
    "  file_info_cols = ['File No.', 'Patient', 'Session', 'File', \n",
    "                        'EEG Type', 'EEG SubType', 'LTM or Routine', \n",
    "                        'Normal/Abnormal', 'No. Seizures File', \n",
    "                        'No. Seizures/Session', 'Filename', 'Seizure Start', \n",
    "                        'Seizure Stop', 'Seizure Type']\n",
    "  file_info.columns = file_info_cols\n",
    "\n",
    "  # we forward fill as there are gaps in the excel file to represent the info \n",
    "  # is the same as above (apart from in the filename, seizure start, seizure stop \n",
    "  # and seizure type columns)\n",
    "  for col_name in file_info.columns[:-4]:\n",
    "    file_info[col_name] = file_info[col_name].ffill()\n",
    "\n",
    "  # patient ID is an integer rather than float\n",
    "  file_info['Patient'] = file_info['Patient'].astype(int)\n",
    "\n",
    "  if seiz_type:\n",
    "    # Get a list of patient event files that have a specifc type of seizure\n",
    "    return list(file_info[file_info['Seizure Type']==seiz_type]['Filename'])\n",
    "  else:\n",
    "    return list(file_info['Filename'])\n",
    "\n",
    "def save_to_database(save_dir, file_title, group, data_x, data_y, \n",
    "                     feature_columns=None):\n",
    "\n",
    "    # open the file in append mode (make it if doesnt exist)\n",
    "    h5file = tables.open_file(save_dir, mode=\"a\", title=file_title)\n",
    "    \n",
    "    # save space\n",
    "    data_x = data_x.astype(np.float32)\n",
    "    #data_y = data_y.astype(np.int16)\n",
    "    \n",
    "    # get filters to compress file\n",
    "    filters = tables.Filters(complevel=1, complib='zlib')\n",
    "    \n",
    "    # if there is already a node for the particpant...\n",
    "    if \"/\"+group in h5file:\n",
    "        # ...put in the directory of where it is found\n",
    "        part_x_array = h5file.get_node(\"/\" + group + '/Data_x')\n",
    "        part_y_array = h5file.get_node(\"/\" + group + '/Data_y')\n",
    "    \n",
    "    else:\n",
    "        # create the group directory\n",
    "        part_group = h5file.create_group(\"/\", group, 'Group Data')\n",
    "        # make an atom which has the datatype found in the data we want to store\n",
    "        x_atom = tables.Atom.from_dtype(data_x.dtype)\n",
    "        y_atom = tables.Atom.from_dtype(data_y.dtype)\n",
    "\n",
    "        if file_title == 'UDWT_Data':\n",
    "          shape = (0,data_x.shape[1], data_x.shape[2], data_x.shape[3])\n",
    "        else:\n",
    "          shape = (0,data_x.shape[1], data_x.shape[2])\n",
    "        \n",
    "        # create an array we can append onto later\n",
    "        part_x_array = h5file.create_earray(\"/\" + group,                   # parentnode\n",
    "                                            'Data_x',                        # name \n",
    "                                            x_atom,                          # atom\n",
    "                                            shape, # shape\n",
    "                                            'Feature Array',\n",
    "                                            filters=filters\n",
    "                                           )                 # title\n",
    "\n",
    "        part_y_array = h5file.create_earray(\"/\" + group, \n",
    "                                            'Data_y', \n",
    "                                            y_atom, \n",
    "                                            (0,),\n",
    "                                            'Events Array',\n",
    "                                            filters=filters\n",
    "                                           )\n",
    "        \n",
    "        if feature_columns:\n",
    "          # create the feature names array (we only need to do this once)\n",
    "          h5file.create_array(\"/\" + group,                                   # where\n",
    "                              'Feature_Names',                             # name \n",
    "                              np.array(feature_columns, dtype='unicode'),  # obj\n",
    "                              \"Names of Each Feature\")                         # title\n",
    "    \n",
    "    # append the data to the array directory\n",
    "    part_x_array.append(data_x)\n",
    "    part_y_array.append(data_y)\n",
    "    \n",
    "    # flush the data to disk\n",
    "    h5file.flush()\n",
    "    # close the file\n",
    "    h5file.close()\n",
    "\n",
    "def udwt_spectrogram(data, waveletname, level, window_size):\n",
    "  data_ucwt = np.ndarray(shape=(data.shape[0], level, data.shape[1], data.shape[2]))\n",
    "  \n",
    "  for ii in range(data.shape[0]):\n",
    "    for jj in range(data.shape[-1]):\n",
    "      signal = data[ii, :, jj]\n",
    "      coeffs_list = swt(signal, waveletname, level=level)\n",
    "      \n",
    "      coeffs_array = np.zeros((len(coeffs_list), window_size))\n",
    "      \n",
    "      for i, array_tuple in enumerate(coeffs_list[::-1]):\n",
    "\n",
    "        coeffs_array[i,:] = np.array(array_tuple)[1,:]\n",
    "      \n",
    "      power = np.abs(coeffs_array)**2\n",
    "      \n",
    "      data_ucwt[ii, :, :, jj] = power\n",
    "      \n",
    "  return data_ucwt\n",
    "\n",
    "\n",
    "if TUH_FILT_OVERWRITE or TUH_FEAT_OVERWRITE or TUH_UDWT_OVERWRITE:\n",
    "\n",
    "  if TUH_FILT_OVERWRITE and os.path.exists(TUH_FILT_SAVE_PATH):\n",
    "    os.remove(TUH_FILT_SAVE_PATH)\n",
    "\n",
    "  if TUH_FEAT_OVERWRITE and os.path.exists(TUH_FEAT_SAVE_PATH):\n",
    "    os.remove(TUH_FEAT_SAVE_PATH)\n",
    "\n",
    "  if TUH_UDWT_OVERWRITE and os.path.exists(TUH_UDWT_SAVE_PATH):\n",
    "    os.remove(TUH_UDWT_SAVE_PATH)\n",
    "\n",
    "  # ---------\n",
    "  # TUH SETUP\n",
    "  # ---------\n",
    "  DOWNLOAD_DIR = \"TUH Database\"\n",
    "  if not os.path.exists(DOWNLOAD_DIR):\n",
    "    os.makedirs(DOWNLOAD_DIR)\n",
    "\n",
    "  user = getpass('TUH Username: ')\n",
    "  key = getpass('TUH Password: ')\n",
    "  auth_dict = {'user': user, 'passwd': key}\n",
    "\n",
    "  # --------------\n",
    "  # GET FILE PATHS\n",
    "  # --------------\n",
    "  # download info files\n",
    "  download_TUH(DOWNLOAD_DIR, auth_dict, '_DOCS')\n",
    "\n",
    "  seiz_types_path = 'dataset/tuh_data/DOCS/seizures_types_v02.xlsx'\n",
    "  seiz_types = pd.read_excel(seiz_types_path)\n",
    "\n",
    "  seiz_types = seiz_types.set_index('Class Code')\n",
    "\n",
    "  int_code = seiz_types.to_dict()['Class No.']\n",
    "  # change to lower case\n",
    "  int_code = { k.lower() : v for k,v in int_code.items() if not isinstance(k, float)}\n",
    "\n",
    "  # get a list of files\n",
    "  tuh_file_list = sel_file_list('train', TUH_code)+sel_file_list('dev_test', TUH_code)\n",
    "\n",
    "  # get a list of the montages\n",
    "  montage = []\n",
    "  for file in tuh_file_list:\n",
    "    montage.append(file.split('/')[3])\n",
    "\n",
    "  # count how many times the montages appear in the data\n",
    "  montage_counts = pd.Series(montage).value_counts()\n",
    "\n",
    "  # remove all files apart from those in the most common montage\n",
    "  regex = re.compile(montage_counts.index[0])\n",
    "  tuh_file_list = [i for i in tuh_file_list if regex.search(i)]\n",
    "  # remove duplicates\n",
    "  tuh_file_list = list(set(tuh_file_list))\n",
    "\n",
    "  # They changed \"dev_test\" to just \"dev\" in their file structure so \n",
    "  # I need to account for this now...\n",
    "  for i, string in enumerate(tuh_file_list):\n",
    "    tuh_file_list[i] = re.sub('_test', '', string)\n",
    "\n",
    "\n",
    "  # --------------------\n",
    "  # GET SIMILAR CHANNELS\n",
    "  # --------------------\n",
    "  # this is to make sure all the data have the same channels\n",
    "  all_channels = []\n",
    "  for events_path in tqdm(tuh_file_list, desc = 'Finding Channels'):\n",
    "    file_ID = events_path.split('/')[-1][:-4]\n",
    "    # we use the above to get the file directory this file is in\n",
    "    pat_file_dir = '/'.join(events_path.split('/')[1:-1])\n",
    "    # this will download all edf and event files for the selected patient\n",
    "    download_TUH(DOWNLOAD_DIR, auth_dict, pat_file_dir, output=True)\n",
    "    \n",
    "    with pyedflib.EdfReader(DOWNLOAD_DIR+'/'+file_ID+'.edf') as f:\n",
    "        # get the names of the signals\n",
    "        all_channels.extend(f.getSignalLabels())\n",
    "\n",
    "  # turn the list into a pandas series\n",
    "  all_channels = pd.Series(all_channels)\n",
    "\n",
    "  # count how many times the channels appear in each participant\n",
    "  channel_counts = all_channels.value_counts()\n",
    "  \n",
    "  # threshold the channels to only those found in all raw data\n",
    "  channel_keeps = list(channel_counts[channel_counts >= channel_counts[0]].index)\n",
    "  regex = re.compile('30|PHOTIC|EKG|PG')\n",
    "  channel_keeps = [i for i in channel_keeps if not regex.search(i)]\n",
    "\n",
    "  # ---------------\n",
    "  # CREATE FEATURES\n",
    "  # ---------------\n",
    "  for events_path in tqdm(tuh_file_list, desc='Creating Features'):\n",
    "    file_ID = events_path.split('/')[-1][:-4]\n",
    "    # we use the above to get the file directory this file is in\n",
    "    pat_file_dir = '/'.join(events_path.split('/')[1:-1])\n",
    "    # patient ID\n",
    "    pat_ID = events_path.split('/')[-3]\n",
    "\n",
    "    # this will download all edf and event files for the selected patient\n",
    "    #download_TUH(DOWNLOAD_DIR, auth_dict, pat_file_dir, output=True)\n",
    "\n",
    "    # load data\n",
    "    raw_data, freq = data_load(DOWNLOAD_DIR+'/'+file_ID+'.edf', channel_keeps)\n",
    "\n",
    "    if raw_data.empty:\n",
    "      print('Skipped: '+file_ID)\n",
    "    else:\n",
    "      raw_events = create_events(DOWNLOAD_DIR+'/'+file_ID+'.tse', raw_data)\n",
    "      # change to integer representation\n",
    "      raw_events = raw_events.replace(int_code)\n",
    "\n",
    "      if TUH_FILT_OVERWRITE or TUH_UDWT_OVERWRITE:\n",
    "\n",
    "        # downsample\n",
    "        down_x, down_y, down_freq = downsample(raw_data.values, raw_events.values, freq)\n",
    "\n",
    "        # window y\n",
    "        window_size = 256*2\n",
    "        overlap = 256\n",
    "        \n",
    "        if TUH_FILT_OVERWRITE:\n",
    "          # filter the data\n",
    "          b, a = signal.butter(4, [1/(down_freq/2), 30/(down_freq/2)], 'bandpass', analog=False)\n",
    "          filt_data = signal.filtfilt(b, a, down_x.T).T\n",
    "          \n",
    "          # scale the data over each channel\n",
    "          SS = StandardScaler()\n",
    "          scaled_data = SS.fit_transform(filt_data)\n",
    "          scaled_data = pd.DataFrame(scaled_data, columns = raw_data.columns, index = down_y)\n",
    "          \n",
    "          # drop na\n",
    "          scaled_data = scaled_data.dropna()\n",
    "          \n",
    "          # window x\n",
    "          data_x = window_x(scaled_data, window_size, overlap)\n",
    "          data_y = window_y(scaled_data.index.values, window_size, overlap, target=None, \n",
    "                            baseline=6)\n",
    "          \n",
    "          # to stop it printing warnings like: object name is not a valid \n",
    "          # Python identifier: '00001113'\n",
    "          with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=tables.NaturalNameWarning)\n",
    "            # save the data\n",
    "            save_to_database(TUH_FILT_SAVE_PATH, 'Filt_Data', pat_ID, data_x, \n",
    "                            data_y[:,0], list(scaled_data.columns))\n",
    "          \n",
    "        # TODO: add in a dropna bit\n",
    "        if TUH_UDWT_OVERWRITE:\n",
    "          # window x\n",
    "          data_x = window_x(pd.DataFrame(down_x, columns = raw_data.columns), \n",
    "                            window_size, overlap)\n",
    "          data_y = window_y(down_y, window_size, overlap, target=None, \n",
    "                          baseline=6)\n",
    "\n",
    "          udwt_data = udwt_spectrogram(data_x, 'db4', 6, window_size)\n",
    "\n",
    "          # get the shape of this data\n",
    "          orig_shape = udwt_data.shape\n",
    "          # reshape to merge the levels and data\n",
    "          udwt_data_reshape = np.reshape(udwt_data, (-1, orig_shape[-1]))\n",
    "          # scale across channels\n",
    "          SS = StandardScaler()\n",
    "          udwt_data_scaled = SS.fit_transform(udwt_data_reshape)\n",
    "          # shape the data back\n",
    "          udwt_data_scaled = np.reshape(udwt_data_scaled, orig_shape)\n",
    "\n",
    "          #display(udwt_data_scaled.shape)\n",
    "\n",
    "          # to stop it printing warnings like: object name is not a valid \n",
    "          # Python identifier: '00001113'\n",
    "          with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=tables.NaturalNameWarning)\n",
    "            # save the data\n",
    "            save_to_database(TUH_UDWT_SAVE_PATH, 'UDWT_Data', \n",
    "                            pat_ID,\n",
    "                            #file_ID, # had to use file ID as appending to an existing array was not working \n",
    "                            udwt_data_scaled, data_y[:,0])\n",
    "\n",
    "        \n",
    "      if TUH_FEAT_OVERWRITE:\n",
    "        # TODO: in 00001795 relative_power = bandpass_2/bandpass_1 creates an inf\n",
    "        # due to dividing by 0. Fix this with a checks in the function, but for now,\n",
    "        # for a quick patch I'll just not use power_ratio.\n",
    "        feat = Seizure_Features(sf = freq,\n",
    "                                window_size=2,\n",
    "                                overlap=1,\n",
    "                                levels=6,\n",
    "                                bandpasses = [[1,4],[4,8],[8,12],\n",
    "                                              [12,30],[30,70]],\n",
    "                                feature_list=['power', #'power_ratio', \n",
    "                                              'mean', 'mean_abs', \n",
    "                                              'std', 'ratio', 'LSWT', 'fft_corr', \n",
    "                                              'fft_eigen', 'time_corr', 'time_eigen'],\n",
    "                                scale = True,\n",
    "                                baseline=6)\n",
    "        \n",
    "\n",
    "        \n",
    "        # just to ignore the runtime warnings about na's\n",
    "        with warnings.catch_warnings():\n",
    "          warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "          \n",
    "          x_feat, y_feat = feat.transform(raw_data.values, \n",
    "                                          raw_events.values,\n",
    "                                          channel_names_list = list(raw_data.columns))\n",
    "\n",
    "        \n",
    "        feat_df = pd.DataFrame(x_feat,\n",
    "                                index=y_feat[:,0], \n",
    "                                columns = feat.feature_names)\n",
    "\n",
    "        feat_df = feat_df.dropna()\n",
    "        \n",
    "        \n",
    "        # to stop it printing warnings like: object name is not a valid \n",
    "        # Python identifier: '00001113'\n",
    "        with warnings.catch_warnings():\n",
    "          warnings.filterwarnings(\"ignore\", category=tables.NaturalNameWarning)\n",
    "          feat_df.to_hdf(TUH_FEAT_SAVE_PATH, \n",
    "                        pat_ID,\n",
    "                        format='table', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:48: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:48: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_10568\\483134918.py:48: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  if re.findall('.xlsx|\\.edf|\\.tse(?!_)', str(link)):\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (483134918.py, line 519)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 519\u001b[1;36m\u001b[0m\n\u001b[1;33m    format='table', append=True) from getpass import getpass\u001b[0m\n\u001b[1;37m                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pyedflib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import tables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "# Import the getpass module\n",
    "from getpass import getpass\n",
    "\n",
    "def find_files(url, headers):\n",
    "    # get a soup of the directory url\n",
    "    soup = BeautifulSoup(requests.get(url, auth=(headers['user'], headers['passwd'])).text, \n",
    "                         features=\"html.parser\")\n",
    "    # make a list of all the links in the url\n",
    "    hrefs_list = []\n",
    "    for link in soup.find_all('a'):\n",
    "        hrefs_list.append(link.get('href'))\n",
    "\n",
    "    return hrefs_list\n",
    "    \n",
    "    \n",
    "def download_file(download_file_url, file_path, headers, output=False):\n",
    "    if output:\n",
    "        # print it is downloading\n",
    "        print('Downloading: '+ download_file_url)\n",
    "    # download the file to the directory\n",
    "    r = requests.get(download_file_url, auth=(headers['user'], headers['passwd']))\n",
    "    with open(file_path, 'wb') as f:\n",
    "      f.write(r.content)\n",
    "\n",
    "# needs a directory to download it to\n",
    "def download_TUH(DIR, headers, sub_dir, output=False):\n",
    "    \n",
    "    # directory url\n",
    "    dir_url = 'https://www.isip.piconepress.com/projects/tuh_eeg/downloads/tuh_eeg_seizure/v1.5.0/'+sub_dir\n",
    "\n",
    "    hrefs_dir_list = find_files(dir_url, headers)\n",
    "    \n",
    "    # for each link in the directory\n",
    "    for link in hrefs_dir_list:\n",
    "        # download the files outside of participant folders we want\n",
    "        if re.findall('.xlsx|\\.edf|\\.tse(?!_)', str(link)):\n",
    "            # if the file doesnt already exist in the directory\n",
    "            if not os.path.exists(os.path.join(DIR, link)):\n",
    "                download_file(dir_url+'/'+str(link), DIR+'/'+str(link), headers, output)\n",
    "\n",
    "def data_load(data_file, selected_channels=[]):\n",
    "\n",
    "    try:\n",
    "        # use the reader to get an EdfReader file\n",
    "        f = pyedflib.EdfReader(data_file)\n",
    "\n",
    "        # get the names of the signals\n",
    "        channel_names = f.getSignalLabels()\n",
    "        # get the sampling frequencies of each signal\n",
    "        channel_freq = f.getSampleFrequencies()\n",
    "        \n",
    "        # get a list of the EEG channels\n",
    "        if len(selected_channels) == 0:\n",
    "            selected_channels = channel_names\n",
    "\n",
    "        # make an empty file of 0's\n",
    "        sigbufs = np.zeros((f.getNSamples()[0],len(selected_channels)))\n",
    "        # for each of the channels in the selected channels\n",
    "        for i, channel in enumerate(selected_channels):\n",
    "        \n",
    "            try:\n",
    "              # add the channel data into the array\n",
    "              sigbufs[:, i] = f.readSignal(channel_names.index(channel))\n",
    "            \n",
    "            except:\n",
    "              ValueError\n",
    "              # This happens if the sampling rate of that channel is \n",
    "              # different to the others.\n",
    "              # For simplicity, in this case we just make it na.\n",
    "              sigbufs[:, i] = np.nan\n",
    "\n",
    "        # turn to a pandas df and save a little space\n",
    "        df = pd.DataFrame(sigbufs, columns = selected_channels).astype('float32')\n",
    "\n",
    "        # get equally increasing numbers upto the length of the data depending\n",
    "        # on the length of the data divided by the sampling frequency\n",
    "        index_increase = np.linspace(0,\n",
    "                                      len(df)/channel_freq[0],\n",
    "                                      len(df), endpoint=False)\n",
    "\n",
    "        # round these to the lowest nearest decimal to get the seconds\n",
    "        seconds = np.floor(index_increase).astype('uint16')\n",
    "\n",
    "        seconds = index_increase\n",
    "        \n",
    "        # make a column the timestamp\n",
    "        df['Time'] = seconds\n",
    "\n",
    "        # make the time stamp the index\n",
    "        df = df.set_index('Time')\n",
    "\n",
    "        # name the columns as channel\n",
    "        df.columns.name = 'Channel'\n",
    "\n",
    "        return df, channel_freq[0]\n",
    "\n",
    "    except OSError as error:\n",
    "        print('Error '+data_file+': '+str(error))\n",
    "        return pd.DataFrame(), None\n",
    "    except ValueError as error:\n",
    "        print('Error '+data_file+': '+str(error))\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "def create_events(file_name, df, code = None):\n",
    "\n",
    "    data_y = pd.Series(index=df.index)\n",
    "    data_y.name = 'Events'\n",
    "\n",
    "    events_tse = pd.read_csv(file_name, \n",
    "                             skiprows=1,\n",
    "                             sep = ' ',\n",
    "                             header=None,\n",
    "                             names =['Start', 'End', 'Code', 'Certainty'])\n",
    "    \n",
    "    data_y = data_y.fillna('bckg')\n",
    "    \n",
    "    for pos, row in events_tse.iterrows():\n",
    "        # if you want to manually set the code\n",
    "        if code != None:\n",
    "          if row['Code'] == code:\n",
    "              data_y[row['Start']:row['End']] = code\n",
    "        # let it be the code it is in the event file\n",
    "        else:\n",
    "          data_y[row['Start']:row['End']] = row['Code']\n",
    "\n",
    "    return data_y\n",
    "\n",
    "def window_y(events, window_size, overlap, target=None, baseline=None):\n",
    "    \n",
    "  # window the data so each row is another epoch\n",
    "  events_windowed = window(events, w = window_size, o = overlap, copy = True)\n",
    "  \n",
    "  if target:\n",
    "    # turn to array of bools if seizure in the\n",
    "    # windowed data\n",
    "    bools = events_windowed == target\n",
    "    # are there any seizure seconds in the data?\n",
    "    data_y = np.any(bools,axis=1)\n",
    "    # turn to 0's and 1's\n",
    "    data_y = data_y.astype(int)\n",
    "    # expand the dimensions so running down one column\n",
    "    data_y = np.expand_dims(data_y, axis=1)\n",
    "  \n",
    "  elif baseline:\n",
    "    # replace all baseline labels to nan\n",
    "    data_y = pd.DataFrame(events_windowed).replace(baseline, np.nan)\n",
    "    # get the most common other than baseline\n",
    "    data_y = data_y.mode(1)\n",
    "    # change nan back to baseline class\n",
    "    data_y = data_y.fillna(baseline).values\n",
    "    # if there was nothing but baseline there will be an empty array\n",
    "    if data_y.size == 0:\n",
    "        data_y = np.array([baseline]*data_y.shape[0])\n",
    "        data_y = np.expand_dims(data_y, -1)\n",
    "  \n",
    "  else:\n",
    "    # get the value most frequent in the window\n",
    "    data_y = pd.DataFrame(events_windowed).mode(1).values\n",
    "\n",
    "  return data_y\n",
    "\n",
    "def downsample(data_x, data_y, freq):\n",
    "    if freq > 256:\n",
    "        if freq >= 1000:\n",
    "          subsample = 4\n",
    "        else:\n",
    "          subsample = 2\n",
    "\n",
    "        freq = freq/subsample\n",
    "        data_x = data_x[::subsample]\n",
    "        data_y = data_y[::subsample]\n",
    "\n",
    "    return data_x, data_y, freq\n",
    "\n",
    "def sel_file_list(set_name, seiz_type):\n",
    "  # load the training information\n",
    "  train_info = pd.read_excel('dataset/tuh_data/DOCS/seizures_types_v02.xlsx', set_name)\n",
    "  # just want the info per file here\n",
    "  file_info = train_info.iloc[1:6101,1:15]\n",
    "  # cleans some of the names\n",
    "  file_info_cols = ['File No.', 'Patient', 'Session', 'File', \n",
    "                        'EEG Type', 'EEG SubType', 'LTM or Routine', \n",
    "                        'Normal/Abnormal', 'No. Seizures File', \n",
    "                        'No. Seizures/Session', 'Filename', 'Seizure Start', \n",
    "                        'Seizure Stop', 'Seizure Type']\n",
    "  file_info.columns = file_info_cols\n",
    "\n",
    "  # we forward fill as there are gaps in the excel file to represent the info \n",
    "  # is the same as above (apart from in the filename, seizure start, seizure stop \n",
    "  # and seizure type columns)\n",
    "  for col_name in file_info.columns[:-4]:\n",
    "    file_info[col_name] = file_info[col_name].ffill()\n",
    "\n",
    "  # patient ID is an integer rather than float\n",
    "  file_info['Patient'] = file_info['Patient'].astype(int)\n",
    "\n",
    "  if seiz_type:\n",
    "    # Get a list of patient event files that have a specifc type of seizure\n",
    "    return list(file_info[file_info['Seizure Type']==seiz_type]['Filename'])\n",
    "  else:\n",
    "    return list(file_info['Filename'])\n",
    "\n",
    "def save_to_database(save_dir, file_title, group, data_x, data_y, \n",
    "                     feature_columns=None):\n",
    "\n",
    "    # open the file in append mode (make it if doesnt exist)\n",
    "    h5file = tables.open_file(save_dir, mode=\"a\", title=file_title)\n",
    "    \n",
    "    # save space\n",
    "    data_x = data_x.astype(np.float32)\n",
    "    #data_y = data_y.astype(np.int16)\n",
    "    \n",
    "    # get filters to compress file\n",
    "    filters = tables.Filters(complevel=1, complib='zlib')\n",
    "    \n",
    "    # if there is already a node for the particpant...\n",
    "    if \"/\"+group in h5file:\n",
    "        # ...put in the directory of where it is found\n",
    "        part_x_array = h5file.get_node(\"/\" + group + '/Data_x')\n",
    "        part_y_array = h5file.get_node(\"/\" + group + '/Data_y')\n",
    "    \n",
    "    else:\n",
    "        # create the group directory\n",
    "        part_group = h5file.create_group(\"/\", group, 'Group Data')\n",
    "        # make an atom which has the datatype found in the data we want to store\n",
    "        x_atom = tables.Atom.from_dtype(data_x.dtype)\n",
    "        y_atom = tables.Atom.from_dtype(data_y.dtype)\n",
    "\n",
    "        if file_title == 'UDWT_Data':\n",
    "          shape = (0,data_x.shape[1], data_x.shape[2], data_x.shape[3])\n",
    "        else:\n",
    "          shape = (0,data_x.shape[1], data_x.shape[2])\n",
    "        \n",
    "        # create an array we can append onto later\n",
    "        part_x_array = h5file.create_earray(\"/\" + group,                   # parentnode\n",
    "                                            'Data_x',                        # name \n",
    "                                            x_atom,                          # atom\n",
    "                                            shape, # shape\n",
    "                                            'Feature Array',\n",
    "                                            filters=filters\n",
    "                                           )                 # title\n",
    "\n",
    "        part_y_array = h5file.create_earray(\"/\" + group, \n",
    "                                            'Data_y', \n",
    "                                            y_atom, \n",
    "                                            (0,),\n",
    "                                            'Events Array',\n",
    "                                            filters=filters\n",
    "                                           )\n",
    "        \n",
    "        if feature_columns:\n",
    "          # create the feature names array (we only need to do this once)\n",
    "          h5file.create_array(\"/\" + group,                                   # where\n",
    "                              'Feature_Names',                             # name \n",
    "                              np.array(feature_columns, dtype='unicode'),  # obj\n",
    "                              \"Names of Each Feature\")                         # title\n",
    "    \n",
    "    # append the data to the array directory\n",
    "    part_x_array.append(data_x)\n",
    "    part_y_array.append(data_y)\n",
    "    \n",
    "    # flush the data to disk\n",
    "    h5file.flush()\n",
    "    # close the file\n",
    "    h5file.close()\n",
    "\n",
    "def udwt_spectrogram(data, waveletname, level, window_size):\n",
    "  data_ucwt = np.ndarray(shape=(data.shape[0], level, data.shape[1], data.shape[2]))\n",
    "  \n",
    "  for ii in range(data.shape[0]):\n",
    "    for jj in range(data.shape[-1]):\n",
    "      signal = data[ii, :, jj]\n",
    "      coeffs_list = swt(signal, waveletname, level=level)\n",
    "      \n",
    "      coeffs_array = np.zeros((len(coeffs_list), window_size))\n",
    "      \n",
    "      for i, array_tuple in enumerate(coeffs_list[::-1]):\n",
    "\n",
    "        coeffs_array[i,:] = np.array(array_tuple)[1,:]\n",
    "      \n",
    "      power = np.abs(coeffs_array)**2\n",
    "      \n",
    "      data_ucwt[ii, :, :, jj] = power\n",
    "      \n",
    "  return data_ucwt\n",
    "\n",
    "\n",
    "if TUH_FILT_OVERWRITE or TUH_FEAT_OVERWRITE or TUH_UDWT_OVERWRITE:\n",
    "\n",
    "  if TUH_FILT_OVERWRITE and os.path.exists(TUH_FILT_SAVE_PATH):\n",
    "    os.remove(TUH_FILT_SAVE_PATH)\n",
    "\n",
    "  if TUH_FEAT_OVERWRITE and os.path.exists(TUH_FEAT_SAVE_PATH):\n",
    "    os.remove(TUH_FEAT_SAVE_PATH)\n",
    "\n",
    "  if TUH_UDWT_OVERWRITE and os.path.exists(TUH_UDWT_SAVE_PATH):\n",
    "    os.remove(TUH_UDWT_SAVE_PATH)\n",
    "\n",
    "  # ---------\n",
    "  # TUH SETUP\n",
    "  # ---------\n",
    "  DOWNLOAD_DIR = \"TUH Database\"\n",
    "  if not os.path.exists(DOWNLOAD_DIR):\n",
    "    os.makedirs(DOWNLOAD_DIR)\n",
    "\n",
    "  user = getpass('TUH Username: ')\n",
    "  key = getpass('TUH Password: ')\n",
    "  auth_dict = {'user': user, 'passwd': key}\n",
    "\n",
    "  # --------------\n",
    "  # GET FILE PATHS\n",
    "  # --------------\n",
    "  # download info files\n",
    "  download_TUH(DOWNLOAD_DIR, auth_dict, '_DOCS')\n",
    "\n",
    "  seiz_types_path = 'dataset/tuh_data/DOCS/seizures_types_v02.xlsx'\n",
    "  seiz_types = pd.read_excel(seiz_types_path)\n",
    "\n",
    "  seiz_types = seiz_types.set_index('Class Code')\n",
    "\n",
    "  int_code = seiz_types.to_dict()['Class No.']\n",
    "  # change to lower case\n",
    "  int_code = { k.lower() : v for k,v in int_code.items() if not isinstance(k, float)}\n",
    "\n",
    "  # get a list of files\n",
    "  tuh_file_list = sel_file_list('train', TUH_code)+sel_file_list('dev_test', TUH_code)\n",
    "\n",
    "  # get a list of the montages\n",
    "  montage = []\n",
    "  for file in tuh_file_list:\n",
    "    montage.append(file.split('/')[3])\n",
    "\n",
    "  # count how many times the montages appear in the data\n",
    "  montage_counts = pd.Series(montage).value_counts()\n",
    "\n",
    "  # remove all files apart from those in the most common montage\n",
    "  regex = re.compile(montage_counts.index[0])\n",
    "  tuh_file_list = [i for i in tuh_file_list if regex.search(i)]\n",
    "  # remove duplicates\n",
    "  tuh_file_list = list(set(tuh_file_list))\n",
    "\n",
    "  # They changed \"dev_test\" to just \"dev\" in their file structure so \n",
    "  # I need to account for this now...\n",
    "  for i, string in enumerate(tuh_file_list):\n",
    "    tuh_file_list[i] = re.sub('_test', '', string)\n",
    "\n",
    "\n",
    "  # --------------------\n",
    "  # GET SIMILAR CHANNELS\n",
    "  # --------------------\n",
    "  # this is to make sure all the data have the same channels\n",
    "  all_channels = []\n",
    "  for events_path in tqdm(tuh_file_list, desc = 'Finding Channels'):\n",
    "    file_ID = events_path.split('/')[-1][:-4]\n",
    "    # we use the above to get the file directory this file is in\n",
    "    pat_file_dir = '/'.join(events_path.split('/')[1:-1])\n",
    "    # this will download all edf and event files for the selected patient\n",
    "    download_TUH(DOWNLOAD_DIR, auth_dict, pat_file_dir, output=True)\n",
    "    \n",
    "    with pyedflib.EdfReader(DOWNLOAD_DIR+'/'+file_ID+'.edf') as f:\n",
    "        # get the names of the signals\n",
    "        all_channels.extend(f.getSignalLabels())\n",
    "\n",
    "  # turn the list into a pandas series\n",
    "  all_channels = pd.Series(all_channels)\n",
    "\n",
    "  # count how many times the channels appear in each participant\n",
    "  channel_counts = all_channels.value_counts()\n",
    "  \n",
    "  # threshold the channels to only those found in all raw data\n",
    "  channel_keeps = list(channel_counts[channel_counts >= channel_counts[0]].index)\n",
    "  regex = re.compile('30|PHOTIC|EKG|PG')\n",
    "  channel_keeps = [i for i in channel_keeps if not regex.search(i)]\n",
    "\n",
    "  # ---------------\n",
    "  # CREATE FEATURES\n",
    "  # ---------------\n",
    "  for events_path in tqdm(tuh_file_list, desc='Creating Features'):\n",
    "    file_ID = events_path.split('/')[-1][:-4]\n",
    "    # we use the above to get the file directory this file is in\n",
    "    pat_file_dir = '/'.join(events_path.split('/')[1:-1])\n",
    "    # patient ID\n",
    "    pat_ID = events_path.split('/')[-3]\n",
    "\n",
    "    # this will download all edf and event files for the selected patient\n",
    "    #download_TUH(DOWNLOAD_DIR, auth_dict, pat_file_dir, output=True)\n",
    "\n",
    "    # load data\n",
    "    raw_data, freq = data_load(DOWNLOAD_DIR+'/'+file_ID+'.edf', channel_keeps)\n",
    "\n",
    "    if raw_data.empty:\n",
    "      print('Skipped: '+file_ID)\n",
    "    else:\n",
    "      raw_events = create_events(DOWNLOAD_DIR+'/'+file_ID+'.tse', raw_data)\n",
    "      # change to integer representation\n",
    "      raw_events = raw_events.replace(int_code)\n",
    "\n",
    "      if TUH_FILT_OVERWRITE or TUH_UDWT_OVERWRITE:\n",
    "\n",
    "        # downsample\n",
    "        down_x, down_y, down_freq = downsample(raw_data.values, raw_events.values, freq)\n",
    "\n",
    "        # window y\n",
    "        window_size = 256*2\n",
    "        overlap = 256\n",
    "        \n",
    "        if TUH_FILT_OVERWRITE:\n",
    "          # filter the data\n",
    "          b, a = signal.butter(4, [1/(down_freq/2), 30/(down_freq/2)], 'bandpass', analog=False)\n",
    "          filt_data = signal.filtfilt(b, a, down_x.T).T\n",
    "          \n",
    "          # scale the data over each channel\n",
    "          SS = StandardScaler()\n",
    "          scaled_data = SS.fit_transform(filt_data)\n",
    "          scaled_data = pd.DataFrame(scaled_data, columns = raw_data.columns, index = down_y)\n",
    "          \n",
    "          # drop na\n",
    "          scaled_data = scaled_data.dropna()\n",
    "          \n",
    "          # window x\n",
    "          data_x = window_x(scaled_data, window_size, overlap)\n",
    "          data_y = window_y(scaled_data.index.values, window_size, overlap, target=None, \n",
    "                            baseline=6)\n",
    "          \n",
    "          # to stop it printing warnings like: object name is not a valid \n",
    "          # Python identifier: '00001113'\n",
    "          with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=tables.NaturalNameWarning)\n",
    "            # save the data\n",
    "            save_to_database(TUH_FILT_SAVE_PATH, 'Filt_Data', pat_ID, data_x, \n",
    "                            data_y[:,0], list(scaled_data.columns))\n",
    "          \n",
    "        # TODO: add in a dropna bit\n",
    "        if TUH_UDWT_OVERWRITE:\n",
    "          # window x\n",
    "          data_x = window_x(pd.DataFrame(down_x, columns = raw_data.columns), \n",
    "                            window_size, overlap)\n",
    "          data_y = window_y(down_y, window_size, overlap, target=None, \n",
    "                          baseline=6)\n",
    "\n",
    "          udwt_data = udwt_spectrogram(data_x, 'db4', 6, window_size)\n",
    "\n",
    "          # get the shape of this data\n",
    "          orig_shape = udwt_data.shape\n",
    "          # reshape to merge the levels and data\n",
    "          udwt_data_reshape = np.reshape(udwt_data, (-1, orig_shape[-1]))\n",
    "          # scale across channels\n",
    "          SS = StandardScaler()\n",
    "          udwt_data_scaled = SS.fit_transform(udwt_data_reshape)\n",
    "          # shape the data back\n",
    "          udwt_data_scaled = np.reshape(udwt_data_scaled, orig_shape)\n",
    "\n",
    "          #display(udwt_data_scaled.shape)\n",
    "\n",
    "          # to stop it printing warnings like: object name is not a valid \n",
    "          # Python identifier: '00001113'\n",
    "          with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=tables.NaturalNameWarning)\n",
    "            # save the data\n",
    "            save_to_database(TUH_UDWT_SAVE_PATH, 'UDWT_Data', \n",
    "                            pat_ID,\n",
    "                            #file_ID, # had to use file ID as appending to an existing array was not working \n",
    "                            udwt_data_scaled, data_y[:,0])\n",
    "\n",
    "        \n",
    "      if TUH_FEAT_OVERWRITE:\n",
    "        # TODO: in 00001795 relative_power = bandpass_2/bandpass_1 creates an inf\n",
    "        # due to dividing by 0. Fix this with a checks in the function, but for now,\n",
    "        # for a quick patch I'll just not use power_ratio.\n",
    "        feat = Seizure_Features(sf = freq,\n",
    "                                window_size=2,\n",
    "                                overlap=1,\n",
    "                                levels=6,\n",
    "                                bandpasses = [[1,4],[4,8],[8,12],\n",
    "                                              [12,30],[30,70]],\n",
    "                                feature_list=['power', #'power_ratio', \n",
    "                                              'mean', 'mean_abs', \n",
    "                                              'std', 'ratio', 'LSWT', 'fft_corr', \n",
    "                                              'fft_eigen', 'time_corr', 'time_eigen'],\n",
    "                                scale = True,\n",
    "                                baseline=6)\n",
    "        \n",
    "\n",
    "        \n",
    "        # just to ignore the runtime warnings about na's\n",
    "        with warnings.catch_warnings():\n",
    "          warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "          \n",
    "          x_feat, y_feat = feat.transform(raw_data.values, \n",
    "                                          raw_events.values,\n",
    "                                          channel_names_list = list(raw_data.columns))\n",
    "\n",
    "        \n",
    "        feat_df = pd.DataFrame(x_feat,\n",
    "                                index=y_feat[:,0], \n",
    "                                columns = feat.feature_names)\n",
    "\n",
    "        feat_df = feat_df.dropna()\n",
    "        \n",
    "        \n",
    "        # to stop it printing warnings like: object name is not a valid \n",
    "        # Python identifier: '00001113'\n",
    "        with warnings.catch_warnings():\n",
    "          warnings.filterwarnings(\"ignore\", category=tables.NaturalNameWarning)\n",
    "          feat_df.to_hdf(TUH_FEAT_SAVE_PATH, \n",
    "                        pat_ID,\n",
    "                        format='table', append=True) from getpass import getpass\n",
    "          \n",
    "import os\n",
    "import sys\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "\n",
    "DOWNLOAD_DIR = os.path.expanduser('tuh_data')  # Set a local path\n",
    "\n",
    "if not os.path.exists(DOWNLOAD_DIR):\n",
    "  os.makedirs(DOWNLOAD_DIR)\n",
    "\n",
    "user = \"nedc-tuh-eeg\"\n",
    "key = \"RLYF8ZhBMZwNnsYA8FsP\"\n",
    "\n",
    "auth_dict = {'user': user, 'passwd': key}\n",
    "\n",
    "download_TUH(DOWNLOAD_DIR, auth_dict, '', output=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
